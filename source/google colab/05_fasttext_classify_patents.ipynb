{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_fasttext_classify_patents.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7n39WeiUy03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78rUk8vCUnLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI-Nw0eWKNVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import csv\n",
        "try:\n",
        "    csv.field_size_limit(sys.maxsize)\n",
        "except:\n",
        "    import unicodecsv\n",
        "    import ctypes\n",
        "    unicodecsv.field_size_limit(int(ctypes.c_ulong(-1).value // 2))\n",
        "    \n",
        "import json\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import stat\n",
        "import glob\n",
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix\n",
        "from datetime import datetime\n",
        "from multiprocessing import Queue, Process\n",
        "import collections\n",
        "import multiprocessing\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import pickle as pkl\n",
        "\n",
        "from sklearn import utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import fasttext\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim.corpora.dictionary import Dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxHOPRQ4Lkpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# txt \n",
        "def get_txt_text(file, length):\n",
        "    for index in range(length):\n",
        "        text = file.readline()\n",
        "    return text\n",
        "\n",
        "def fill_dataframe(data_frame, classifications_df, classcode, abstract, claim, description):\n",
        "    if classcode != \"\":\n",
        "        # shrink the set to only_top_classes? TRUE/FALSE\n",
        "        classcode = cut_down(classcode, 4, ['H', 'B', 'C'], False) # H, B, C\n",
        "        data_frame.loc[data_frame.shape[0] + 1] = [abstract, claim, description, classcode]\n",
        "        classifications_df = calculate_class_distribution(classcode, classifications_df)\n",
        "\n",
        "def handle_patent_file(data_frame, classifications_df, path_filename):\n",
        "    file = open(path_filename, \"r\")\n",
        "\n",
        "    kind = get_txt_text(file, 1).strip()\n",
        "    if kind == 'A1':\n",
        "        classcode = get_txt_text(file, 1).strip()\n",
        "        applicant = get_txt_text(file, 1).strip()\n",
        "\n",
        "        abstract = get_txt_text(file, 1).strip()\n",
        "        citations = get_txt_text(file, 1).strip()\n",
        "        file.close()\n",
        "\n",
        "        fill_dataframe(data_frame, classifications_df, classcode, abstract, None, None)\n",
        "    elif kind == 'B1':\n",
        "        classcode = get_txt_text(file, 1).strip()\n",
        "        id_respective_document = get_txt_text(file, 1).strip()\n",
        "\n",
        "        # abstract = get_txt_text(file, 1).strip()\n",
        "        claim = get_txt_text(file, 1).strip()\n",
        "        description = get_txt_text(file, 1).strip()\n",
        "        # citations = get_txt_text(file, 1).strip()\n",
        "        file.close()\n",
        "\n",
        "        fill_dataframe(data_frame, classifications_df, classcode, None, claim, description)\n",
        "    else:\n",
        "        if kind == 'A1B1':\n",
        "            print(\"eu_mix_patent !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        else:\n",
        "            print(\"us_patent     !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        classcode = get_txt_text(file, 1).strip()\n",
        "        applicant = get_txt_text(file, 1).strip()\n",
        "\n",
        "        abstract = get_txt_text(file, 1).strip()\n",
        "        claim = get_txt_text(file, 1).strip()\n",
        "        description = get_txt_text(file, 1).strip()\n",
        "        citations = get_txt_text(file, 1).strip()\n",
        "        file.close()\n",
        "\n",
        "        fill_dataframe(data_frame, classifications_df, classcode, abstract, claim, description)\n",
        "    return get_patent_id(path_filename)\n",
        "\n",
        "def handle_path_patent(data_frame, classifications_df, path):\n",
        "    return list(map(lambda path_filename : handle_patent_file(data_frame, classifications_df, path_filename), get_list_files(path, 'txt')))\n",
        "\n",
        "def load_data(source_path):\n",
        "    print('###  reading patents  ###')\n",
        "    \"\"\" load_data \"\"\"\n",
        "    data_frame = pd.DataFrame(columns=['abstract', 'claim', 'description', 'classification'])\n",
        "    classifications_df = pd.DataFrame(columns=['class', 'count'])\n",
        "    patent_ids = []\n",
        "\n",
        "    patent_ids = list(map(lambda path : handle_path_patent(data_frame, classifications_df, path), source_path))\n",
        "    patent_ids = get_flat_list(patent_ids)\n",
        "\n",
        "    data_frame['id'] = data_frame.index\n",
        "    classifications_df.sort_values(by=['count'], ascending=False, inplace=True, kind='quicksort')\n",
        "    return patent_ids, data_frame, classifications_df\n",
        "\n",
        "def handle_row(row, ids_list):\n",
        "    if isinstance(row, pd.Series):\n",
        "        try:\n",
        "            id_, patent_id, text, classcodes = row.tolist()\n",
        "        except:\n",
        "            patent_id, text, classcodes = row.tolist()\n",
        "        tokens = tokenize_text(text)\n",
        "        if len(tokens) < 2:\n",
        "            ids_list.append(patent_id)\n",
        "        else:\n",
        "            if isinstance(classcodes, str):\n",
        "                temp_classcodes = tokenize_text(classcodes)\n",
        "                for class_ in temp_classcodes:\n",
        "                    if len(class_) == 4:\n",
        "                        if class_[0] in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] and class_[1].isdigit() and class_[2].isdigit() and class_[3].isalpha():\n",
        "                            pass\n",
        "                        else:\n",
        "                            ids_list.append(patent_id)\n",
        "                            break\n",
        "                    else:\n",
        "                        ids_list.append(patent_id)\n",
        "                        break\n",
        "            else:\n",
        "                print('not string')\n",
        "\n",
        "def check_out_for_whitespaces(text):\n",
        "    if isinstance(text, str):\n",
        "        return ' '.join([element for element in tokenize_text(text) if len(element) > 2 and len(element) < 31])\n",
        "\n",
        "def check_out_empty_texts_and_wrong_classcodes(data_frame):\n",
        "    print('dataframe shape: ', data_frame.shape)\n",
        "    ids_list = []\n",
        "    data_frame.apply(lambda row : handle_row(row, ids_list), axis=1)  \n",
        "    data_frame.set_index(data_frame['patent_id'], inplace=True)\n",
        "    \n",
        "    data_frame.drop(ids_list, axis=0, inplace=True)\n",
        "\n",
        "    data_frame['text'] = data_frame['text'].apply(lambda text : check_out_for_whitespaces(text))\n",
        "    print('ids_list: ', ids_list)\n",
        "    print('dataframe shape: ', data_frame.shape)\n",
        "    return data_frame\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "def get_final_df(patent_ids, temp_df, classif_type):\n",
        "    classification_target = 'description_claim_abstract_title'\n",
        "\n",
        "    data_frame = pd.DataFrame(columns=['id_', 'patent_id', 'text', 'classification'])\n",
        "\n",
        "    if temp_df.shape[1] == 5:\n",
        "        data_frame['patent_id'] = temp_df['patent_id']\n",
        "    else:\n",
        "        data_frame[\"patent_id\"] = patent_ids\n",
        "\n",
        "    # textual_information = temp_df[\"abstract\"]\n",
        "    # textual_information = temp_df[\"claim\"]\n",
        "    # textual_information = temp_df[\"description\"]\n",
        "    textual_information = temp_df[\"abstract\"].str.cat(temp_df[\"claim\"], sep =\" \", na_rep=\" \").str.cat(temp_df[\"description\"], sep =\" \", na_rep=\" \")\n",
        "    data_frame['text'] = textual_information\n",
        "\n",
        "    # data_frame['text'] = data_frame['text'].apply(lambda text : clean_text(text))\n",
        "    data_frame = check_out_empty_texts_and_wrong_classcodes(data_frame)\n",
        "\n",
        "    classification_types = get_classifications(temp_df)\n",
        "    data_frame[\"classification\"] = classification_types[classif_type]\n",
        "    return data_frame, classification_target, classif_type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kywSA6LLnGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# folder\n",
        "def create_folder(destination_path):\n",
        "    if not os.path.exists(destination_path):\n",
        "        os.makedirs(destination_path)\n",
        "\n",
        "def get_list_files(path, extension):\n",
        "    if extension:\n",
        "        return glob.glob(path + '*.' + extension)\n",
        "    return glob.glob(path)\n",
        "\n",
        "def link_paths(root_path, ending_path):\n",
        "    return os.path.join(root_path, ending_path)\n",
        "\n",
        "def join_paths(root_path, ending_path):\n",
        "    path = link_paths(root_path, ending_path)\n",
        "    create_folder(path)\n",
        "    return path\n",
        "\n",
        "def get_root_location(ending_path):\n",
        "    return join_paths('/content/drive/My Drive/Colab Notebooks/', ending_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtCJrEdeLovO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tool\n",
        "def cut_down(classification, index, top_classes, only_top_classes):\n",
        "    temp_list = tokenize_text(classification)\n",
        "    if only_top_classes:\n",
        "        new_classification_list = list(set(filter(lambda element : element[:index] in top_classes, temp_list)))\n",
        "        new_classification = get_string_from_list(new_classification_list, ' ')\n",
        "    else:\n",
        "        new_classification_list = list(set(map(lambda element : element[:index], temp_list)))\n",
        "        new_classification = get_string_from_list(new_classification_list, ' ')\n",
        "    return new_classification\n",
        "\n",
        "def calculate_class_distribution(classification, classifications_df):\n",
        "    for _class in tokenize_text(classification):\n",
        "        if classifications_df['class'].str.contains(_class).any():\n",
        "            index = classifications_df.index[classifications_df['class'] == _class]\n",
        "            classifications_df.loc[index[0], ['count']] += 1\n",
        "        else:\n",
        "            classifications_df.loc[classifications_df.shape[0] + 1] = [_class, 1]\n",
        "    return classifications_df\n",
        "\n",
        "def get_flat_list(list_of_lists):\n",
        "    return [element for elements in list_of_lists for element in elements]\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "def tokenize_complex_text_in_set(text):\n",
        "    return set(map(lambda word : word, tokenize_text(text)))\n",
        "\n",
        "def get_set_from_index(index_set, data_set):\n",
        "    set_ = np.ndarray(shape=(index_set.shape[0], data_set.shape[1]))\n",
        "    for index, data_index in enumerate(index_set['text'].values):\n",
        "        set_[index-1] = data_set[data_index-1]\n",
        "    return set_, index_set['patent_id']\n",
        "\n",
        "def get_string_from_list(list_, linking_string):\n",
        "    return linking_string.join(element for element in list_ if isinstance(element, str))\n",
        "\n",
        "def get_patent_id(string):\n",
        "    index = string.rfind('/')\n",
        "    return string[index+1:-4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rmBaWmdPWmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word\n",
        "def train_doc2vec(data_, model_path):\n",
        "    cores = multiprocessing.cpu_count()\n",
        "    model_dbow = Doc2Vec(dm=0, vector_size=150, negative=5, min_count=1, alpha=0.25, min_alpha=0.05, sample=0, workers=cores)\n",
        "    model_dbow.build_vocab([x for x in tqdm(data_)])\n",
        "    date = datetime.datetime.now().isoformat()\n",
        "    model_dbow.save(link_paths(model_path, 'doc2vec_model '+date))\n",
        "    return model_dbow\n",
        "\n",
        "class Dov2VecHelper():\n",
        "    def __init__(self):\n",
        "        print('###  doc2vec_vectorizer  ###')\n",
        "        self.labeled = []\n",
        "        self.vectors = []\n",
        "\n",
        "    def label_sentences(self, corpus, label_type):\n",
        "        \"\"\"\n",
        "        Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
        "        We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
        "        a dummy index of the post.\n",
        "        \"\"\"\n",
        "        self.labeled = []\n",
        "        for i, v in enumerate(corpus):\n",
        "            label = label_type + '_' + str(i)\n",
        "            self.labeled.append(TaggedDocument(th.tokenize_text(v), [label]))\n",
        "        return self.labeled\n",
        "\n",
        "    def get_vectors(self, model, corpus_size, vectors_size, vectors_type):\n",
        "        \"\"\"\n",
        "        Get vectors from trained doc2vec model\n",
        "        :param doc2vec_model: Trained Doc2Vec model\n",
        "        :param corpus_size: Size of the data\n",
        "        :param vectors_size: Size of the embedding vectors\n",
        "        :param vectors_type: Training or Testing vectors\n",
        "        :return: list of vectors\n",
        "        \"\"\"\n",
        "        self.vectors = np.zeros((corpus_size, vectors_size))\n",
        "        for i in range(0, corpus_size):\n",
        "            prefix = vectors_type + '_' + str(i)\n",
        "            self.vectors[i] = model.docvecs[prefix]\n",
        "        return self.vectors\n",
        "\n",
        "def get_word2vec_model(data_, out_path):\n",
        "    cores = multiprocessing.cpu_count()\n",
        "    # results_word2vec_sg_1_size_150_hs_1_cbow_mean_1_negative_5_min_count_1_lpha_0.25_min_alpha_0\n",
        "    # results_word2vec_sg_0_size_150_hs_1_cbow_mean_1_negative_12_min_count_1_lpha_0.25_min_alpha_0\n",
        "    model_dbow = Word2Vec(data_, sg=1, size=150, hs=1, cbow_mean=1, negative=5, min_count=1, alpha=.25, min_alpha=.0, compute_loss=True, window=5, workers=cores)\n",
        "    # model_dbow.build_vocab([x for x in tqdm(data_)])\n",
        "\n",
        "    for epoch in range(30):\n",
        "        model_dbow.train(utils_shuffle_rows([x for x in tqdm(data_)]), total_examples=len(data_), epochs=1)\n",
        "        model_dbow.alpha -= 0.002\n",
        "        model_dbow.min_alpha = model_dbow.alpha\n",
        "\n",
        "    date = datetime.datetime.now().isoformat()\n",
        "    model_dbow.save(link_paths(out_path, 'word2vec_model ' + date))\n",
        "    return model_dbow\n",
        "\n",
        "class Word2VecHelper():\n",
        "    def __init__(self):\n",
        "        print('###  word2vec_vectorizer  ###')\n",
        "        self.tokens = []\n",
        "        self.mean = []\n",
        "\n",
        "        self.vectors = []\n",
        "\n",
        "    def w2v_tokenize_text(self, text):\n",
        "        self.tokens = []\n",
        "        for word in tokenize_text(text):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            self.tokens.append(word)\n",
        "        return self.tokens\n",
        "\n",
        "    def word_averaging(self, wv, words):\n",
        "        all_words = set()\n",
        "        self.mean = []\n",
        "\n",
        "        for word in words:\n",
        "            if isinstance(word, np.ndarray):\n",
        "                self.mean.append(word)\n",
        "            elif word in wv.vocab:\n",
        "                self.mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "                all_words.add(wv.vocab[word].index)\n",
        "\n",
        "        if not self.mean:\n",
        "            print(\"POSSIBLE ERROR IN WORD2VECHELPER: cannot compute similarity with no input %s\", words)\n",
        "            return np.zeros(wv.vector_size,)\n",
        "\n",
        "        self.mean = gensim.matutils.unitvec(np.array(self.mean).mean(axis=0)).astype(np.float32)\n",
        "        return self.mean\n",
        "\n",
        "    def  word_averaging_list(self, wv, text_list):\n",
        "        return np.vstack([self.word_averaging(wv, post) for post in text_list])\n",
        "\n",
        "    def reduce_dimensions(model, X_data):\n",
        "        # num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
        "\n",
        "        vectors = [] # positions in vector space\n",
        "        new_element = []\n",
        "        # labels = [] # keep track of words to label our data again later\n",
        "        for element in X_data:\n",
        "            print(\"element: \", element)\n",
        "            for word in element:\n",
        "                new_element.append(model.wv[word])\n",
        "                # labels.append(word)\n",
        "            vectors.append(new_element)\n",
        "\n",
        "        # convert both lists into numpy vectors for reduction\n",
        "        vectors = np.asarray(vectors)\n",
        "        # labels = np.asarray(labels)\n",
        "\n",
        "        # reduce using t-SNE\n",
        "        # vectors = np.asarray(vectors)\n",
        "        # tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
        "        # vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "        x_vals = [v[0] for v in vectors]\n",
        "        # y_vals = [v[1] for v in vectors]\n",
        "        return x_vals #, y_vals, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X53LNyX8LrIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classification\n",
        "def get_distinct_class_substrings(classification_list, first_int, second_int):\n",
        "    list_ = []\n",
        "    for element in classification_list:\n",
        "        for class_ in tokenize_text(element):\n",
        "            if class_[first_int:second_int] not in list_:\n",
        "                list_.append(class_[first_int:second_int])\n",
        "    list_.sort()\n",
        "    return list_\n",
        "\n",
        "# def apply_method_to_get_classes(element):\n",
        "#     list_ = list(map(lambda class_ : class_, th.tokenize_text(element)))\n",
        "#     return \" \".join(item for item in list(set(list_)))\n",
        "\n",
        "# def get_alternative_class_substrings(classification_list, first_int, second_int):\n",
        "#     list_ = list(map(lambda element : apply_method_to_get_classes(element), classification_list))\n",
        "#     return list_\n",
        "\n",
        "def get_class_substrings(classification_list, first_int, second_int):\n",
        "    list_ = []\n",
        "    for element in classification_list:\n",
        "        string = \"\"\n",
        "        for class_ in tokenize_text(element):\n",
        "            if class_[first_int:second_int] not in string:\n",
        "                string += class_[first_int:second_int] + ' '\n",
        "        list_.append(string)\n",
        "    return list_\n",
        "\n",
        "def get_classifications(temp_df):\n",
        "    ## Load utility data\n",
        "    classifications = temp_df['classification'].tolist()\n",
        "\n",
        "    valid_sections = get_distinct_class_substrings(classifications, 0, 1)\n",
        "    valid_classes = get_distinct_class_substrings(classifications, 1, 3)\n",
        "    valid_subclasses = get_distinct_class_substrings(classifications, 3, 4)\n",
        "\n",
        "    sections = get_class_substrings(classifications, 0, 1)\n",
        "    classes = get_class_substrings(classifications, 0, 3)\n",
        "    subclasses = get_class_substrings(classifications, 0, 4)\n",
        "\n",
        "    classification_types = {\n",
        "        \"sections\": sections,\n",
        "        \"classes\": classes,\n",
        "        \"subclasses\": subclasses\n",
        "    }\n",
        "    return classification_types\n",
        "\n",
        "def get_train_test_from_data(X, Y):\n",
        "    return train_test_split(X, Y, random_state=0, test_size=.2, shuffle=True)\n",
        "\n",
        "def get_train_test_from_dataframe(data_frame):\n",
        "    return train_test_split(data_frame, random_state=0, test_size=.2, shuffle=True)\n",
        "\n",
        "def save_sets(sets_location, train_data, test_data, val_data, train_labels, test_labels, val_labels, settings, date='01-01-2020'):\n",
        "    try:  \n",
        "        sets_location = join_paths(sets_location, 'tensorflow ' + tf.version.VERSION)\n",
        "        actual_sets_location = join_paths(sets_location, date)\n",
        "        with open(link_paths(actual_sets_location, 'training_set '+date+'.pkl'), \"wb\") as f:\n",
        "            pkl.dump([train_data, train_labels], f)\n",
        "        with open(link_paths(actual_sets_location, 'testing_set '+date+'.pkl'), \"wb\") as f:\n",
        "            pkl.dump([test_data, test_labels], f)\n",
        "        with open(link_paths(actual_sets_location, 'validation_set '+date+'.pkl'), \"wb\") as f:\n",
        "            pkl.dump([val_data, val_labels], f)\n",
        "        with open(link_paths(actual_sets_location, 'settings '+date+'.pkl'), \"wb\") as f:\n",
        "            pkl.dump(settings, f)\n",
        "    except:\n",
        "        print('A problem occurred while saving the sets!')\n",
        "\n",
        "def load_sets(sets_location, date='01-01-2020'):\n",
        "    try:\n",
        "        sets_location = join_paths(sets_location, 'tensorflow ' + tf.version.VERSION)\n",
        "        actual_sets_location = join_paths(sets_location, date)\n",
        "        with open(link_paths(actual_sets_location, 'training_set '+date+'.pkl'), \"rb\") as f:\n",
        "            train_data, train_labels = pkl.load(f)\n",
        "        with open(link_paths(actual_sets_location, 'testing_set '+date+'.pkl'), \"rb\") as f:\n",
        "            test_data, test_labels = pkl.load(f)\n",
        "        with open(link_paths(actual_sets_location, 'validation_set '+date+'.pkl'), \"rb\") as f:\n",
        "            val_data, val_labels = pkl.load(f)\n",
        "        with open(link_paths(actual_sets_location, 'settings '+date+'.pkl'), \"rb\") as f:\n",
        "            try:\n",
        "                settings = pkl.load(f)\n",
        "                # classes, n_classes, vocab_processor, len_vocabulary = pkl.load(f)\n",
        "            except:\n",
        "                settings = [None, None, None, None]\n",
        "        return train_data, test_data, val_data, train_labels, test_labels, val_labels, settings\n",
        "    except:\n",
        "        print('A problem occurred while loading the sets!')\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "def save_data_frame(script_key, data_frame, csvfile):\n",
        "    path_to_csv = get_root_location('model_dataframe/')\n",
        "    date = datetime.now().isoformat()\n",
        "    if csvfile:\n",
        "        output_path = fh.link_paths(path_to_csv, csvfile)\n",
        "    else:\n",
        "        output_path = fh.link_paths(path_to_csv, 'dataframe '+date+'.csv')\n",
        "    print('output_path: ', output_path)\n",
        "    \n",
        "    data_frame.to_csv(output_path, index=False, sep=',', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "\n",
        "def load_data_frame(script_key, csvfile):\n",
        "    path_to_csv = get_root_location('model_dataframe/')\n",
        "    if csvfile:\n",
        "        input_path = link_paths(path_to_csv, csvfile)\n",
        "    else:\n",
        "        input_path = link_paths(path_to_csv, 'dataframe '+date+'.csv')\n",
        "    print('input_path: ', input_path)\n",
        "    \n",
        "    data_frame = pd.read_csv(input_path, sep=',', quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \", header=None, engine='python')\n",
        "    data_frame.columns = ['patent_id', 'text', 'classification']\n",
        "    \n",
        "    # data_frame['text'] = data_frame['text'].apply(lambda text : clean_text(text))\n",
        "    data_frame = check_out_empty_texts_and_wrong_classcodes(data_frame)\n",
        "    \n",
        "    classification_df = pd.DataFrame(columns=['class', 'count'])\n",
        "    data_frame['classification'].apply(lambda classcode : calculate_class_distribution(classcode, classification_df))\n",
        "    return data_frame, classification_df\n",
        "\n",
        "# as a list of strings, which are consequently made of labels (as above separated by an empty space)\n",
        "def apply_simple_binarizer(classification, classes):\n",
        "    print('###  label_binarizer  ###')\n",
        "    binarized_classification = label_binarize(classification, classes=classes)\n",
        "    return binarized_classification, classes, binarized_classification.shape[1]\n",
        "\n",
        "# TODO: may be the same as the simple\n",
        "def apply_label_binarizer(classification):\n",
        "    lb = LabelBinarizer()\n",
        "    return lb.fit_transform(classification)\n",
        "\n",
        "def apply_multilabel_binarizer(data_frame):\n",
        "    print('###  multi_label_binarizer  ###')\n",
        "    ################################################ classification: from text to sparse binary matrix [[0, 1, 0],[1, 0, 1]]\n",
        "    temp_classification = data_frame.apply(lambda row : tokenize_complex_text_in_set(row['classification']), axis=1)\n",
        "    df_to_list = temp_classification.tolist()\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    mlb.fit(df_to_list)\n",
        "    classes = list(mlb.classes_)\n",
        "    return mlb.transform(df_to_list), classes, len(classes)\n",
        "\n",
        "def apply_tfidf_vectorizer_fit(data_frame):\n",
        "    print('###  tfidf_vectorizer_text  ###')\n",
        "    vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
        "    vectorizer.fit(data_frame['text'].apply(lambda x : np.str_(x)))\n",
        "    return vectorizer\n",
        "\n",
        "def apply_tfidf_vectorizer_transform(text, vectorizer):\n",
        "    return vectorizer.transform(text.apply(lambda x : np.str_(x)))\n",
        "\n",
        "def apply_word2vec_word_averaging(data_frame):\n",
        "    print('###  word2vec_word_averaging_vectorizer  ###')\n",
        "    helper_word2vec = Word2VecHelper()\n",
        "    text_tokenized = data_frame.apply(lambda row : helper_word2vec.w2v_tokenize_text(row['text']), axis=1).values\n",
        "    if False:\n",
        "        try:\n",
        "            path_ = join_paths(current_path[:current_path.rfind('/', 0, -8)], \"/data/GoogleNews-vectors-negative300.bin\")\n",
        "            print(\"google vectors : \", path_)\n",
        "            wv = gensim.models.KeyedVectors.load_word2vec_format(path_, binary=True)\n",
        "        except:\n",
        "            print(\"unable to find word2vec model from google, downloading...\")\n",
        "            wv = gensim.models.KeyedVectors.load_word2vec_format(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "            print(\"done...\")\n",
        "    else:\n",
        "        model_path = join_paths(current_path[:current_path.rfind('/', 0, -8)], \"data/vectorizer_models\")\n",
        "        model = get_word2vec_model(text_tokenized, model_path)\n",
        "        wv = model.wv\n",
        "    wv.init_sims(replace=True)\n",
        "    return helper_word2vec.word_averaging_list(wv, text_tokenized)\n",
        "\n",
        "def apply_doc2vec(data_frame, type_, doc2vec_path):\n",
        "    Y, classes, n_classes = apply_classification_vectorizer(type_, data_frame)\n",
        "    print('finisched class vect')\n",
        "    X_train, X_test, y_train, y_test = get_train_test_from_data(data_frame, Y)\n",
        "    print('finisched get train test')\n",
        "    patent_ids = X_train['patent_id']\n",
        "    print('finisched ids')\n",
        "    helper_doc2vec = Dov2VecHelper()\n",
        "    print('finisched doc2vec helpers')\n",
        "    X_train = helper_doc2vec.label_sentences(X_train['text'], 'Train')\n",
        "    print('finisched label sent')\n",
        "    X_test = helper_doc2vec.label_sentences(X_test['text'], 'Test')\n",
        "    print('finisched label sent')\n",
        "    all_data = X_train + X_test\n",
        "    print('finisched all data')\n",
        "    model_dbow = train_doc2vec(all_data, X_train, X_test, y_train, y_test, n_classes, doc2vec_path)\n",
        "    print('finisched train doc2vec')\n",
        "    train_vectors_dbow = helper_doc2vec.get_vectors(model_dbow, len(X_train), 150, 'Train')\n",
        "    print('finisched get vect')\n",
        "    test_vectors_dbow = helper_doc2vec.get_vectors(model_dbow, len(X_test), 150, 'Test')\n",
        "    print('finisched get vect')\n",
        "    return train_vectors_dbow, test_vectors_dbow,  y_train, y_test, classes, n_classes, patent_ids\n",
        "\n",
        "def apply_standard_vectorizer(data_frame, type_):\n",
        "    Y, classes, n_classes = apply_classification_vectorizer(type_, data_frame)\n",
        "    X, vocab_processor, len_vocabulary = apply_vocabulary_processor(data_frame['text'])\n",
        "    # a list of the words used in the text, identified by a unique number for each different word\n",
        "\n",
        "    X_train, X_test, y_train, y_test = get_train_test_from_data(X, Y)\n",
        "\n",
        "    print(\"Vocabulary Size: {:d}\".format(len_vocabulary))\n",
        "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
        "    return X_train, X_test, y_train, y_test, classes, n_classes, vocab_processor, len_vocabulary\n",
        "\n",
        "def set_string_for_fasttext(item):\n",
        "    return str(item).strip().replace(' or ', ' ').replace(', or ', ' ').replace(', ', ' ').replace(',', ' __label__').replace('$$', ' ').replace('$', ' ').replace(' ', ' __label__').replace('___', '__')\n",
        "\n",
        "def apply_classification_vectorizer(type_, data_frame):\n",
        "    classification = data_frame['classification']\n",
        "    if type_ == 'simple':\n",
        "        classes = ['H', 'B', 'C'] # useful if i need to shrink the set of classes\n",
        "        return apply_simple_binarizer(classification, classes)\n",
        "    elif type_ == 'label_binarizer':\n",
        "        return apply_label_binarizer(classification), None, 0\n",
        "    elif type_ == 'multi_label':\n",
        "        return apply_multilabel_binarizer(data_frame)\n",
        "    elif type_ == 'fasttext':\n",
        "        return ['__label__'+set_string_for_fasttext(item) for item in data_frame['classification']], None, 0\n",
        "    return classification, None, 0\n",
        "\n",
        "def apply_df_vectorizer(data_frame, type_, classification_type, model_name):\n",
        "    model_path = '/Users/elio/Desktop/Patent-Classification/data/vectorizer_models/'\n",
        "    # type_ = \"doc2vec\"\n",
        "    # classification_type = \"multi_label\"\n",
        "    if type_ == 'doc2vec':\n",
        "        X_train, X_test, y_train, y_test, classes, n_classes, patent_ids = apply_doc2vec(data_frame, classification_type, model_path)\n",
        "        # array - not numpy\n",
        "        print('finisched apply doc2vec')\n",
        "        vocab_processor, len_vocabulary = None, 0\n",
        "    elif type_ == 'standard':\n",
        "        standard_results = apply_standard_vectorizer(data_frame, classification_type)\n",
        "        X_train, X_test, y_train, y_test, classes, n_classes, vocab_processor, len_vocabulary = standard_results\n",
        "        patent_ids = data_frame['patent_id']\n",
        "    else:\n",
        "        Y_vect, classes, n_classes = apply_classification_vectorizer(classification_type, data_frame)\n",
        "        vocab_processor, len_vocabulary = None, 0\n",
        "        if type_ == 'tfidf_fit_transform': # not sure it remains the same order\n",
        "            X_vect = pd.DataFrame({'text' : data_frame['text'], 'patent_id' : data_frame['patent_id']})\n",
        "            X_train, X_test, y_train, y_test = get_train_test_from_data(X_vect, Y_vect)\n",
        "\n",
        "            patent_ids = X_train['patent_id']\n",
        "            print('almost started tfidf')\n",
        "            vectorizer = apply_tfidf_vectorizer_fit(data_frame)\n",
        "            print('applied tfidf')\n",
        "            X_train = apply_tfidf_vectorizer_transform(X_train['text'], vectorizer)\n",
        "            print('transformed train')\n",
        "            X_test = apply_tfidf_vectorizer_transform(X_test['text'], vectorizer)\n",
        "            print('transformed test')\n",
        "            # csr matrix\n",
        "        elif type_ == 'word2vec': # required google pre-trained vectors, it starts downloading if you don't have\n",
        "            X_vect = apply_word2vec_word_averaging(data_frame)\n",
        "            # ndarray\n",
        "            print('finished word2vec word averaging')\n",
        "            text_indexes = data_frame.index.values\n",
        "\n",
        "            X_vect_temp = pd.DataFrame({'text' : text_indexes, 'patent_id' : data_frame['patent_id']})\n",
        "            X_train_temp, X_test_temp, y_train, y_test = get_train_test_from_data(X_vect_temp, Y_vect)\n",
        "\n",
        "            X_train, patent_ids = get_set_from_index(X_train_temp, X_vect)\n",
        "            X_test, _ = get_set_from_index(X_test_temp, X_vect)\n",
        "            print('finished word2vec vectorizer')\n",
        "        else:\n",
        "            data_frame.drop(columns=['classification'])\n",
        "            X_train, X_test, y_train, y_test = get_train_test_from_data(data_frame, Y_vect)\n",
        "            return X_train, X_test, y_train, y_test, classes, n_classes, vocab_processor, len_vocabulary\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, classes, n_classes, vocab_processor, len_vocabulary\n",
        "\n",
        "def get_fasttext_classifier_information(text):\n",
        "    return text.split('.')[1]\n",
        "\n",
        "def get_metrics_values(metrics):\n",
        "    return list(map(lambda tuple_ : apply_method_to_create_metrics(tuple_), metrics))\n",
        "\n",
        "def get_sequential_layers_values(parameters):\n",
        "    return [list(map(lambda token : token, th.tokenize_text(parameters[:-1])))]\n",
        "\n",
        "def get_sequential_LSTM_metrics_values(metrics):\n",
        "    micro = [\"precision:\"+str(metrics[\"precision_micro\"]), \"recall:\"+str(metrics[\"recall_micro\"]), \"f1:\"+str(metrics[\"f1_micro\"])]\n",
        "    macro = [\"precision:\"+str(metrics[\"precision_macro\"]), \"recall:\"+str(metrics[\"recall_macro\"]), \"f1:\"+str(metrics[\"f1_macro\"])]\n",
        "    tops = [\"top_1:\"+str(metrics[\"top_1\"]), \"top_3:\"+str(metrics[\"top_3\"]), \"top_5:\"+str(metrics[\"top_5\"])]\n",
        "    return [str(metrics[\"coverage_error\"]), micro, macro, tops]\n",
        "\n",
        "def get_parameters_values(parameters):\n",
        "    parameters.replace(\" \", \"\")\n",
        "    return list(map(lambda token : token.split('=')[1], parameters.split(',')))\n",
        "\n",
        "def get_parameters_list(model, metrics, parameters, classif_approach, classif_level, classif_type, dataset_location):\n",
        "    list_ = [model]\n",
        "    list_ += list(map(lambda item : item, metrics))\n",
        "    list_ += [classif_approach, classif_level, classif_type]\n",
        "    list_ += list(map(lambda item : item, parameters))\n",
        "    list_ += [dataset_location]\n",
        "    return list_\n",
        "\n",
        "def get_saving_dataframe(model_key, metrics_values, parameters_values, classif_approach, classif_level, classif_type, dataset_location):\n",
        "    columns_list = get_parameters_list('model_name', settings[model_key][\"metrics\"], settings[model_key][\"parameters\"], 'approach', 'level', 'type', 'dataset_location')\n",
        "    values_list = get_parameters_list(model_key, metrics_values, parameters_values, classif_approach, classif_level, classif_type, dataset_location)\n",
        "\n",
        "    data_frame = pd.DataFrame(columns=columns_list)\n",
        "    data_frame.loc[data_frame.shape[0] + 1] = values_list\n",
        "    return data_frame\n",
        "\n",
        "def write_dataframe_as_csv(data_frame, path_to_csv):\n",
        "    if os.path.isfile(path_to_csv):\n",
        "        with open(path_to_csv, 'a') as f:\n",
        "            data_frame.to_csv(f, sep=',', header=False)\n",
        "    else:\n",
        "        data_frame.to_csv(path_to_csv, sep=',', header=True)\n",
        "\n",
        "settings = {\n",
        "            \"FastText\" : {\n",
        "                          \"results_file_name\" : \"data/models_results/sequantial_results.csv\",\n",
        "                          \"metrics\" : ['precision', 'recall'],\n",
        "                          \"parameters\" : ['parameters'],\n",
        "            }\n",
        "}\n",
        "\n",
        "def save_results(model_key, metrics, parameters, classif_approach, classif_level, classif_type, dataset_location):\n",
        "    print('###  saving_results  ###')\n",
        "    root_location = get_root_location('data/fasttext_outcome/')\n",
        "    path_to_csv = link_paths(join_paths(root_location, 'model_results'), 'results.csv')\n",
        "\n",
        "    if model_key == 'Sequential':\n",
        "        metrics = list(map(lambda metric : [metric], metrics))\n",
        "        # metrics = [[metric] for metric in metrics]\n",
        "        metrics_values = get_metrics_values(metrics)\n",
        "        parameters_values = get_sequential_layers_values(parameters)\n",
        "    elif model_key == 'Sequential Test':\n",
        "        metrics = [[-1], [-1], [-1], # metrics['loss'], metrics['accuracy'], metrics['mse'],\n",
        "                   [metrics['precision_micro'], metrics['recall_micro'], metrics['f1_micro']],\n",
        "                   [metrics['precision_macro'], metrics['recall_macro'], metrics['f1_macro']]]\n",
        "        metrics_values = get_metrics_values(metrics)\n",
        "        parameters_values = get_sequential_layers_values(parameters)\n",
        "    elif model_key == 'FastText':\n",
        "        metrics_values = metrics[1:3]\n",
        "        parameters_values = [parameters]\n",
        "    elif model_key == 'Sequential_LSTM':\n",
        "        temp_metrics = metrics\n",
        "        del temp_metrics[\"total_positive\"], temp_metrics[\"average_num_of_labels\"]\n",
        "        metrics_values = get_sequential_LSTM_metrics_values(temp_metrics)\n",
        "        parameters_values = get_sequential_layers_values(parameters)\n",
        "    else:\n",
        "        metrics_values = get_metrics_values(metrics)\n",
        "        parameters_values = get_parameters_values(parameters)\n",
        "\n",
        "    data_frame = get_saving_dataframe(model_key, metrics_values, parameters_values, classif_approach, classif_level, classif_type, dataset_location)\n",
        "    write_dataframe_as_csv(data_frame, path_to_csv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eUOyvN0ohCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# metrics helper\n",
        "def display_directly_metrics(algorithm, accuracy, precision, recall, f1):\n",
        "    print('@@@  ', algorithm ,'  @@@')\n",
        "\n",
        "    print('test accuracy : {0:0.5f}'.format(accuracy))\n",
        "    print('test precision : {0:0.5f}'.format(precision))\n",
        "    print('test recall : {0:0.5f}'.format(recall))\n",
        "    print('test f1 : {0:0.5f}'.format(f1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QbrrIV9KoYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "script_key = \"fasttext classify\"\n",
        "\n",
        "def preprocessing_data_for_fasttext(data_frame, text_vectorizer, class_vectorizer):\n",
        "    root_location = get_root_location('data/fasttext_outcome/')\n",
        "\n",
        "    data_frame['text'] = data_frame['text'].replace('\\n',' ', regex=True).replace('\\t',' ', regex=True)\n",
        "\n",
        "    model_name = text_vectorizer+'/'+class_vectorizer+'/FastText'\n",
        "    X_train, X_test, Y_train, Y_test, _, _, _, _ = apply_df_vectorizer(data_frame, text_vectorizer, class_vectorizer, model_name)\n",
        "\n",
        "    X_train, X_val, Y_train, Y_val = get_train_test_from_data(X_train, Y_train)\n",
        "    # try:\n",
        "    if not isinstance(X_train, pd.DataFrame):\n",
        "        train = pd.DataFrame(data=X_train)\n",
        "        test = pd.DataFrame(data=X_test)\n",
        "        val = pd.DataFrame(data=X_val)\n",
        "    else:\n",
        "        train = X_train\n",
        "        test = X_test\n",
        "        val = X_val\n",
        "\n",
        "    train.loc[:, 1] = Y_train\n",
        "    test.loc[:, 1] = Y_test\n",
        "    val.loc[:, 1] = Y_val\n",
        "\n",
        "    print(train.head(5))\n",
        "    \n",
        "    train.drop(columns=['patent_id'], inplace=True)\n",
        "    test.drop(columns=['patent_id'], inplace=True)\n",
        "    val.drop(columns=['patent_id'], inplace=True)\n",
        "    \n",
        "    val.to_csv(link_paths(root_location, 'validating set.csv'), index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\",escapechar=\" \")\n",
        "    train.to_csv(link_paths(root_location, 'training set.csv'), index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\",escapechar=\" \")\n",
        "    test.to_csv(link_paths(root_location, 'testing set.csv'), index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "\n",
        "    print('csv saved')\n",
        "    # except:\n",
        "    #     print('a problem occurred while trying to store the dataframes')\n",
        "\n",
        "    #     val = pd.DataFrame({'text': X_val, 'classification': Y_val})\n",
        "    #     train = pd.DataFrame({'text': X_train, 'classification': Y_train})\n",
        "    #     test = pd.DataFrame({'text': X_test, 'classification': Y_test})\n",
        "\n",
        "    #     data_frame.to_csv(link_paths(root_location, 'dataframe.csv'), index=False, sep=' ', header=False,quoting=csv.QUOTE_NONE,quotechar=\"\",escapechar=\" \")\n",
        "\n",
        "    #     val.to_csv(link_paths(root_location, 'validating set.csv'), index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\",escapechar=\" \")\n",
        "    #     train.to_csv(link_paths(root_location, 'training set.csv'), index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\",escapechar=\" \")\n",
        "    #     test.to_csv(link_paths(root_location, 'testing set.csv'), index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "        \n",
        "    #     print('csv saved')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PPsYFFcKTEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_fasttext(data_frame, text_vectorizer, class_vectorizer, classif_level, classif_type, dataset_location):\n",
        "    root_location = get_root_location('data/fasttext_outcome/')\n",
        "\n",
        "    fasttext_location = join_paths(root_location, 'fasttext_models/')\n",
        "    word2vec_location = join_paths(root_location, 'word2vec_models/')\n",
        "    timestamp = datetime.now().isoformat()\n",
        "\n",
        "    # model = fasttext.train_supervised(input=link_paths(root_location, 'training set.csv'), autotuneValidationFile=link_paths(root_location, 'validating set.csv'), verbose=3)\n",
        "\n",
        "    model = fasttext.train_supervised(input=link_paths(root_location, 'training set.csv'), dim=300, minCount=1, minn=0, maxn=0, epoch=30, lr=.618225, loss='softmax', bucket=937478, wordNgrams=3, \n",
        "                                      pretrainedVectors='/content/drive/My Drive/Colab Notebooks/data/crawl-300d-2M.vec')\n",
        "    parameters = '[dim=300, minCount=1, minn=0, maxn=0, epoch=30, lr=.618225, loss=softmax, bucket=937478, wordNgrams=3]'\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    y_pred = np.array([])\n",
        "    texts = pd.read_csv(link_paths(root_location, 'training set.csv'))\n",
        "    for index, row in texts.iterrows():\n",
        "        if isinstance(row, pd.Series):\n",
        "            text = row.tolist()\n",
        "            # print('shrink ', text[-15:])\n",
        "            possible_labels = text[-15:][0]\n",
        "            y_predicted, metrics_array = model.predict(text, k=-1, threshold=.05)\n",
        "            np.append(y_pred, y_predicted)\n",
        "            total += 1\n",
        "            # print(tokenize_text(possible_labels))\n",
        "            lbl = list(filter(lambda x: x.lower().startswith(\"__label__\"), tokenize_text(possible_labels)))\n",
        "            # print(lbl, y_predicted[0])\n",
        "            total += len(lbl)-1\n",
        "            # print('toal: ', total)\n",
        "            correct += len(list(set(y_predicted[0]).intersection(lbl)))\n",
        "\n",
        "    results = model.test(link_paths(root_location, 'testing set.csv'), k=-1)\n",
        "\n",
        "    result_top_5 = model.test(link_paths(root_location, 'testing set.csv'), k=5)\n",
        "    result_top_3 = model.test(link_paths(root_location, 'testing set.csv'), k=3)\n",
        "    result_top_1 = model.test(link_paths(root_location, 'testing set.csv'), k=1)\n",
        "\n",
        "    classifier_name = get_fasttext_classifier_information(str(model))\n",
        "    model_name = text_vectorizer+'/'+class_vectorizer+'/'+classifier_name\n",
        "\n",
        "    display_directly_metrics('k=-1 '+classifier_name, -1, results[1], results[2], 2*(results[1]*results[2])/(results[1]+results[2]))\n",
        "    display_directly_metrics('k= 5 '+classifier_name, -1, result_top_5[1], result_top_5[2], 2*(result_top_5[1]*result_top_5[2])/(result_top_5[1]+result_top_5[2]))\n",
        "    display_directly_metrics('k= 3 '+classifier_name, -1, result_top_3[1], result_top_3[2], 2*(result_top_3[1]*result_top_3[2])/(result_top_3[1]+result_top_3[2]))\n",
        "    display_directly_metrics('k= 1 '+classifier_name, -1, result_top_1[1], result_top_1[2], 2*(result_top_1[1]*result_top_1[2])/(result_top_1[1]+result_top_1[2]))\n",
        "\n",
        "    save_results(classifier_name, results, parameters, model_name, classif_level, classif_type, dataset_location)\n",
        "\n",
        "    print(y_pred)\n",
        "\n",
        "    print(correct/total, correct, total)\n",
        "\n",
        "    print(model.labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crgWxULWKdGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shrink_vocabulary(row, vocabulary, data_frame, ids_list):\n",
        "    if isinstance(row, pd.Series):\n",
        "        patent_id, text, classification = row.tolist()\n",
        "\n",
        "        new_tokens = [element for element in text if element in vocabulary]\n",
        "        if new_tokens != [] or len(new_tokens) > 2:\n",
        "            data_frame.loc[data_frame.shape[0]+1] = [patent_id, ' '.join(element for element in new_tokens), classification]\n",
        "        else:\n",
        "            ids_list.append(patent_id)\n",
        "\n",
        "def further_preprocessing_phase(temp_data_frame):\n",
        "    temp_data_frame['text'] = temp_data_frame['text'].apply(lambda text: th.tokenize_text(text) if text != None else '')\n",
        "    # textlist = temp_data_frame['text'].to_numpy()\n",
        "    textlist = temp_data_frame['text'].tolist()\n",
        "\n",
        "    # if it raises an exeption could be the empty texts\n",
        "    patent_dictionary = Dictionary(textlist)\n",
        "    corpus = [patent_dictionary.doc2bow(text) for text in textlist]\n",
        "\n",
        "    print('original dictionary size: ', len(patent_dictionary))\n",
        "\n",
        "    vocab_tf={}\n",
        "    for i in corpus:\n",
        "        for item, count in dict(i).items():\n",
        "            if item in vocab_tf:\n",
        "                vocab_tf[item]+=int(count)\n",
        "            else:\n",
        "                vocab_tf[item] =int(count)\n",
        "\n",
        "    remove_ids=[]\n",
        "    no_of_ids_below_limit=0\n",
        "    for id,count in vocab_tf.items():\n",
        "        if count<=5:\n",
        "            remove_ids.append(id)\n",
        "    patent_dictionary.filter_tokens(bad_ids=remove_ids)\n",
        "\n",
        "    patent_dictionary.filter_extremes(no_below=0)\n",
        "    patent_dictionary.filter_n_most_frequent(30)\n",
        "\n",
        "    print('parsed dictionary size: ', len(patent_dictionary))\n",
        "\n",
        "    vocabulary = list(patent_dictionary.token2id.keys())\n",
        "\n",
        "    ids_list = []\n",
        "    data_frame = pd.DataFrame(columns=['patent_id', 'text', 'classification'])\n",
        "    temp_data_frame.apply(lambda row : shrink_vocabulary(row, vocabulary, data_frame, ids_list), axis=1)\n",
        "    print(len(ids_list))\n",
        "    data_frame.set_index(data_frame['patent_id'], inplace=True)\n",
        "    data_frame.drop(ids_list, axis=0, inplace=True)\n",
        "    return data_frame\n",
        "\n",
        "def shrink_to_sectors(classcode_list):\n",
        "    new_list = []\n",
        "    for class_ in classcode_list:\n",
        "        if class_[0] not in new_list:\n",
        "            new_list.append(class_[0])\n",
        "    return ' '.join(element for element in new_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # source_path = ['/Users/elio/Desktop/Patent-Classification/data/test_classification/cleaned/B/']\n",
        "    source_path = ['/content/drive/My Drive/Colab Notebooks/data/test_classification/B 1500 patents/']\n",
        "    source_path = ['/content/drive/My Drive/Colab Notebooks/data/test_classification/B 500 patents/']\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    text_vectorizer = 'None'\n",
        "    class_vectorizer = 'fasttext'\n",
        "    classif_level = 'description_claim_abstract_title'\n",
        "    classif_type = 'subclasses'\n",
        "\n",
        "    # patent_ids, temp_df, classifications_df = load_data(source_path)\n",
        "    # data_frame, classif_level, classif_type = get_final_df(patent_ids, temp_df, classif_type)\n",
        "\n",
        "    csvfile = 'training_2000_2004_cleaned.csv'\n",
        "    # csvfile = 'training_2000_2004_cleaned_test.csv'\n",
        "    csvfile = 'training_2000_2004_cleaned_test_smaller.csv'\n",
        "\n",
        "    classif_level, classif_type = 'description_claim_abstract_title', 'subclasses'\n",
        "    training_df, classification_df = load_data_frame(script_key, csvfile)\n",
        "    \n",
        "    csvfile = 'testing_2005_cleaned.csv'\n",
        "    # csvfile = 'testing_2005_cleaned_test.csv'\n",
        "    csvfile = 'testing_2005_cleaned_test_smaller.csv'\n",
        "    testing_df, classification_df = load_data_frame(script_key, csvfile)\n",
        "\n",
        "    data_frame = pd.concat([training_df, testing_df])\n",
        "    patent_ids = data_frame['patent_id'].tolist()\n",
        "    \n",
        "    print('dataframe shape: ', data_frame.shape)\n",
        "\n",
        "    data_frame = further_preprocessing_phase(data_frame)\n",
        "    # data_frame['classification'] = data_frame['classification'].apply(lambda classcode : shrink_to_sectors(tokenize_text(classcode)))\n",
        "\n",
        "    preprocessing_data_for_fasttext(data_frame, text_vectorizer, class_vectorizer)\n",
        "\n",
        "    apply_fasttext(_, text_vectorizer, class_vectorizer, classif_level, classif_type, source_path)\n",
        "\n",
        "    print(\"end fasttext classification step, execution time: \", time.time()-start)\n",
        "\n",
        "# TODO list:\n",
        "# try all the state-of-the art and maybe we can combine two of them.\n",
        "#\n",
        "# little trouble: i have to save two CSVs (one for testing one for training) in order to feed the model with them. It doesn't work directly with dataframes. is it normal?\n",
        "#\n",
        "# quite big trouble: the only way to specify the model for learning the word representation (cbow or skipgram) is using the unsupervised learning with the argument \"model\". There isn't the possibility to add it to the supervised. if i apply the vectorization before it works with impressively much less time.\n",
        "#\n",
        "# k means for? predictions\n",
        "#\n",
        "# gensim ha a fasttext implementation"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}