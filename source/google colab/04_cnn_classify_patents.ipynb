{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "04_cnn_classify_patents.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oYcfj03tVX3k",
        "outputId": "e5fe85f9-cda9-4482-eb50-fc8db3eab380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4VonCFDNY-8",
        "outputId": "a2919dd2-14e5-4bea-c561-ae9a2162aa29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "!pip install fasttext\n",
        "# !pip install q keras==2.2.5\n",
        "# !pip install tensorflow==2.0\n",
        "# !pip install --upgrade --force-reinstall tensorflow-gpu \n",
        "# !pip install -Uq tensorflow-transform==0.15.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.17.5)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (45.2.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MivvP8ylYKo-",
        "outputId": "2fc0ecf3-e3f2-4073-eec8-3f5f809a4452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "# %tensorflow_version 2.x\n",
        "\n",
        "import sys\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "try:\n",
        "    csv.field_size_limit(sys.maxsize)\n",
        "except:\n",
        "    import unicodecsv\n",
        "    import ctypes\n",
        "    unicodecsv.field_size_limit(int(ctypes.c_ulong(-1).value // 2))\n",
        "    \n",
        "from itertools import product\n",
        "import os\n",
        "import stat\n",
        "import time\n",
        "import pickle as pkl\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.version.VERSION)\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import glob\n",
        "from multiprocessing import Queue, Process\n",
        "\n",
        "import gensim\n",
        "\n",
        "from sklearn import utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from keras.layers import Input, Dense, Dropout, Activation\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers.convolutional import MaxPooling1D, Convolution1D\n",
        "from keras.layers.recurrent import LSTM\n",
        "\n",
        "import fasttext\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import Callback\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "import collections\n",
        "import multiprocessing\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import coverage_error\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Iv6Xt4DtZzzx",
        "colab": {}
      },
      "source": [
        "# folder helper\n",
        "def get_list_files(path, extension):\n",
        "    if extension:\n",
        "        return glob.glob(path + '*.' + extension)\n",
        "    return glob.glob(path)\n",
        "\n",
        "def create_folder(destination_path):\n",
        "    if not os.path.exists(destination_path):\n",
        "        os.makedirs(destination_path)\n",
        "\n",
        "def ensure_exists_path_location(path):\n",
        "    if os.path.exists(path):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_root_location(ending_path):\n",
        "    return join_paths('/content/drive/My Drive/Colab Notebooks/', ending_path)\n",
        "\n",
        "def link_paths(root_path, ending_path):\n",
        "    return os.path.join(root_path, ending_path)\n",
        "\n",
        "def join_paths(root_path, ending_path):\n",
        "    path = link_paths(root_path, ending_path)\n",
        "    create_folder(path)\n",
        "    return path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K7S_Qs7naLOi",
        "colab": {}
      },
      "source": [
        "# txt helpers\n",
        "def handle_partial_args(source_path):\n",
        "    source_path = link_paths(source_path, '*')\n",
        "    source_path = get_list_files(source_path, None)\n",
        "    source_path = list(map(lambda path : path + '/', source_path))\n",
        "    print(\"source path: %s\" % source_path)\n",
        "\n",
        "    if len(source_path) == 0 or source_path[len(source_path)-1][-5:-1] == '.xml':\n",
        "        return th.source_path_warnings()\n",
        "    else:\n",
        "        folder_level = source_path[0].count('/')-1\n",
        "        print(\"folder destination level: %s\" % folder_level)\n",
        "        return source_path, folder_level\n",
        "\n",
        "def get_txt_text(file, length):\n",
        "    for index in range(length):\n",
        "        text = file.readline()\n",
        "    return text\n",
        "\n",
        "def fill_dataframe(data_frame, classifications_df, id_, classcode, abstract, claim, description):\n",
        "    if classcode != \"\":\n",
        "        # shrink the set to only_top_classes? TRUE/FALSE\n",
        "        classcode = cut_down(classcode, 4, ['H', 'B', 'C'], False) # H, B, C\n",
        "        data_frame.loc[data_frame.shape[0] + 1] = [id_, abstract, claim, description, classcode]\n",
        "        classifications_df = calculate_class_distribution(classcode, classifications_df)\n",
        "\n",
        "def handle_patent_file(data_frame, classifications_df, path_filename):\n",
        "    file = open(path_filename, \"r\")\n",
        "    id_ = get_patent_id(path_filename)\n",
        "    kind = get_txt_text(file, 1).strip()\n",
        "    if kind == 'A1':\n",
        "        classcode = get_txt_text(file, 1).strip()\n",
        "        applicant = get_txt_text(file, 1).strip()\n",
        "\n",
        "        abstract = get_txt_text(file, 1).strip()\n",
        "        citations = get_txt_text(file, 1).strip()\n",
        "        file.close()\n",
        "\n",
        "        fill_dataframe(data_frame, classifications_df, id_, classcode, abstract, None, None)\n",
        "    elif kind == 'B1':\n",
        "        classcode = get_txt_text(file, 1).strip()\n",
        "        id_respective_document = get_txt_text(file, 1).strip()\n",
        "\n",
        "        # abstract = get_txt_text(file, 1).strip()\n",
        "        claim = get_txt_text(file, 1).strip()\n",
        "        description = get_txt_text(file, 1).strip()\n",
        "        # citations = get_txt_text(file, 1).strip()\n",
        "        file.close()\n",
        "\n",
        "        fill_dataframe(data_frame, classifications_df, id_, classcode, None, claim, description)\n",
        "    else:\n",
        "        if kind == 'A1B1':\n",
        "            print(\"eu_mix_patent !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        else:\n",
        "            print(\"us_patent     !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        classcode = get_txt_text(file, 1).strip()\n",
        "        applicant = get_txt_text(file, 1).strip()\n",
        "\n",
        "        abstract = get_txt_text(file, 1).strip()\n",
        "        claim = get_txt_text(file, 1).strip()\n",
        "        description = get_txt_text(file, 1).strip()\n",
        "        citations = get_txt_text(file, 1).strip()\n",
        "        file.close()\n",
        "\n",
        "        fill_dataframe(data_frame, classifications_df, id_, classcode, abstract, claim, description)\n",
        "    return id_\n",
        "\n",
        "def handle_path_patent(data_frame, classifications_df, path):\n",
        "    return list(map(lambda path_filename : handle_patent_file(data_frame, classifications_df, path_filename), get_list_files(path, 'txt')))\n",
        "\n",
        "def load_data(source_path):\n",
        "    print('###  reading patents  ###')\n",
        "    \"\"\" load_data \"\"\"\n",
        "    data_frame = pd.DataFrame(columns=['patent_id', 'abstract', 'claim', 'description', 'classification'])\n",
        "    classifications_df = pd.DataFrame(columns=['class', 'count'])\n",
        "    patent_ids = []\n",
        "\n",
        "    patent_ids = list(map(lambda path : handle_path_patent(data_frame, classifications_df, path), tqdm(source_path)))\n",
        "    patent_ids = get_flat_list(patent_ids)\n",
        "\n",
        "    data_frame['id'] = data_frame.index\n",
        "    classifications_df.sort_values(by=['count'], ascending=False, inplace=True, kind='quicksort')\n",
        "    return patent_ids, data_frame, classifications_df\n",
        "\n",
        "def handle_row(row, ids_list):\n",
        "    if isinstance(row, pd.Series):\n",
        "        try:\n",
        "            id_, patent_id, text, classcodes = row.tolist()\n",
        "        except:\n",
        "            patent_id, text, classcodes = row.tolist()\n",
        "        tokens = tokenize_text(text)\n",
        "        if len(tokens) < 2:\n",
        "            ids_list.append(patent_id)\n",
        "        else:\n",
        "            if isinstance(classcodes, str):\n",
        "                temp_classcodes = tokenize_text(classcodes)\n",
        "                for class_ in temp_classcodes:\n",
        "                    if len(class_) == 4:\n",
        "                        if class_[0] in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] and class_[1].isdigit() and class_[2].isdigit() and class_[3].isalpha():\n",
        "                            pass\n",
        "                        else:\n",
        "                            ids_list.append(patent_id)\n",
        "                            break\n",
        "                    else:\n",
        "                        ids_list.append(patent_id)\n",
        "                        break\n",
        "            else:\n",
        "                print('not string')\n",
        "\n",
        "def check_out_for_whitespaces(text):\n",
        "    if isinstance(text, str):\n",
        "        return ' '.join([element for element in tokenize_text(text) if len(element) > 2 and len(element) < 31])\n",
        "\n",
        "def check_out_empty_texts_and_wrong_classcodes(data_frame):\n",
        "    print('dataframe shape: ', data_frame.shape)\n",
        "    ids_list = []\n",
        "    data_frame.apply(lambda row : handle_row(row, ids_list), axis=1)  \n",
        "    data_frame.set_index(data_frame['patent_id'], inplace=True)\n",
        "    \n",
        "    data_frame.drop(ids_list, axis=0, inplace=True)\n",
        "\n",
        "    data_frame['text'] = data_frame['text'].apply(lambda text : check_out_for_whitespaces(text))\n",
        "    print('ids_list: ', ids_list)\n",
        "    print('dataframe shape: ', data_frame.shape)\n",
        "    return data_frame\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "def get_final_df(patent_ids, temp_df, classif_type):\n",
        "    classification_target = 'description_claim_abstract_title'\n",
        "\n",
        "    data_frame = pd.DataFrame(columns=['id_', 'patent_id', 'text', 'classification'])\n",
        "\n",
        "    if temp_df.shape[1] == 5:\n",
        "        data_frame['patent_id'] = temp_df['patent_id']\n",
        "    else:\n",
        "        data_frame[\"patent_id\"] = patent_ids\n",
        "\n",
        "    # textual_information = temp_df[\"abstract\"]\n",
        "    # textual_information = temp_df[\"claim\"]\n",
        "    # textual_information = temp_df[\"description\"]\n",
        "    textual_information = temp_df[\"abstract\"].str.cat(temp_df[\"claim\"], sep =\" \", na_rep=\" \").str.cat(temp_df[\"description\"], sep =\" \", na_rep=\" \")\n",
        "    data_frame['text'] = textual_information\n",
        "\n",
        "    # data_frame['text'] = data_frame['text'].apply(lambda text : clean_text(text))\n",
        "    data_frame = check_out_empty_texts_and_wrong_classcodes(data_frame)\n",
        "\n",
        "    classification_types = get_classifications(temp_df)\n",
        "    data_frame[\"classification\"] = classification_types[classif_type]\n",
        "    return data_frame, classification_target, classif_type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bLIYyGD3aXOp",
        "colab": {}
      },
      "source": [
        "#  tool helper\n",
        "def get_patent_id(string):\n",
        "    index = string.rfind('/')\n",
        "    return string[index+1:-4]\n",
        "\n",
        "def get_flat_list(list_of_lists):\n",
        "    return [element for elements in list_of_lists for element in elements]\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "def tokenize_complex_text_in_set(text):\n",
        "    return set(map(lambda word : word, tokenize_text(text)))\n",
        "\n",
        "def get_string_from_list(list_, linking_string):\n",
        "    return linking_string.join(element for element in list_ if isinstance(element, str))\n",
        "\n",
        "def get_set_from_index(index_set, data_set):\n",
        "    set_ = np.ndarray(shape=(index_set.shape[0], data_set.shape[1]))\n",
        "    for index, data_index in enumerate(index_set['text'].values):\n",
        "        set_[index-1] = data_set[data_index-1]\n",
        "    return set_, index_set['patent_id']\n",
        "\n",
        "def cut_down(classification, index, top_classes, only_top_classes):\n",
        "    temp_list = tokenize_text(classification)\n",
        "    if only_top_classes:\n",
        "        new_classification_list = list(set(filter(lambda element : element[:index] in top_classes, temp_list)))\n",
        "        new_classification = get_string_from_list(new_classification_list, ' ')\n",
        "    else:\n",
        "        new_classification_list = list(set(map(lambda element : element[:index], temp_list)))\n",
        "        new_classification = get_string_from_list(new_classification_list, ' ')\n",
        "    return new_classification\n",
        "\n",
        "def calculate_class_distribution(classification, classifications_df):\n",
        "    if isinstance(classification, str):\n",
        "        for _class in tokenize_text(classification):\n",
        "            if classifications_df['class'].str.contains(_class).any():\n",
        "                index = classifications_df.index[classifications_df['class'] == _class]\n",
        "                classifications_df.loc[index[0], ['count']] += 1\n",
        "            else:\n",
        "                classifications_df.loc[classifications_df.shape[0] + 1] = [_class, 1]\n",
        "        return classifications_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1zjXYYUpa1qH",
        "colab": {}
      },
      "source": [
        "# classification helper\n",
        "def get_train_test_from_data(X, Y):\n",
        "    return train_test_split(X, Y, random_state=0, test_size=.1, shuffle=True)\n",
        "\n",
        "def get_train_test_from_dataframe(data_frame):\n",
        "    return train_test_split(data_frame, random_state=0, test_size=.1, shuffle=True)\n",
        "\n",
        "def get_distinct_class_substrings(classification_list, first_int, second_int):\n",
        "    list_ = []\n",
        "    for element in classification_list:\n",
        "        for class_ in tokenize_text(element):\n",
        "            if class_[first_int:second_int] not in list_:\n",
        "                list_.append(class_[first_int:second_int])\n",
        "    list_.sort()\n",
        "    return list_\n",
        "\n",
        "def get_class_substrings(classification_list, first_int, second_int):\n",
        "    list_ = []\n",
        "    for element in classification_list:\n",
        "        sub_list = []\n",
        "        for class_ in tokenize_text(element):\n",
        "            if class_[first_int:second_int] not in sub_list:\n",
        "                sub_list.append(class_[first_int:second_int])\n",
        "        string = \"\".join(element for element in sub_list)\n",
        "        list_.append(string)\n",
        "    return list_\n",
        "\n",
        "def get_classifications(temp_df):\n",
        "    ## Load utility data\n",
        "    classifications = temp_df['classification'].tolist()\n",
        "\n",
        "    valid_sections = get_distinct_class_substrings(classifications, 0, 1)\n",
        "    valid_classes = get_distinct_class_substrings(classifications, 1, 3)\n",
        "    valid_subclasses = get_distinct_class_substrings(classifications, 3, 4)\n",
        "\n",
        "    sections = get_class_substrings(classifications, 0, 1)\n",
        "    classes = get_class_substrings(classifications, 0, 3)\n",
        "    subclasses = get_class_substrings(classifications, 0, 4)\n",
        "\n",
        "    classification_types = {\n",
        "        \"sections\": sections,\n",
        "        \"classes\": classes,\n",
        "        \"subclasses\": subclasses\n",
        "    }\n",
        "    return classification_types\n",
        "\n",
        "def save_training_set(training_set, model_name):\n",
        "    print('###  saving_training_set  ###')\n",
        "    root_location = get_root_location('data/model_training_sets/')\n",
        "    model_name = model_name.replace(\"/\", \"-\")\n",
        "    path = link_paths(root_location, 'training '+model_name+' '+str(datetime.now())[:-10]+'.npy')\n",
        "    np.save(path, training_set)\n",
        "\n",
        "def save_sets(sets_location, train_data, test_data, val_data, train_labels, test_labels, val_labels, settings, date='01-01-2020'):\n",
        "    try:  \n",
        "        sets_location = join_paths(sets_location, 'tensorflow ' + tf.version.VERSION)\n",
        "        actual_sets_location = join_paths(sets_location, date)\n",
        "        with open(link_paths(actual_sets_location, 'training_set '+date+'.pkl'), \"wb\") as f:\n",
        "            pkl.dump([train_data, train_labels], f)\n",
        "        with open(link_paths(actual_sets_location, 'testing_set '+date+'.pkl'), \"wb\") as f:\n",
        "            pkl.dump([test_data, test_labels], f)\n",
        "        with open(link_paths(actual_sets_location, 'validation_set '+date+'.pkl'), \"wb\") as f:\n",
        "            pkl.dump([val_data, val_labels], f)\n",
        "        with open(link_paths(actual_sets_location, 'settings '+date+'.pkl'), \"wb\") as f:\n",
        "            pkl.dump(settings, f)\n",
        "    except:\n",
        "        print('A problem occurred while saving the sets!')\n",
        "\n",
        "def load_sets(sets_location, date='01-01-2020'):\n",
        "    try:\n",
        "        sets_location = join_paths(sets_location, 'tensorflow ' + tf.version.VERSION)\n",
        "        actual_sets_location = join_paths(sets_location, date)\n",
        "        with open(link_paths(actual_sets_location, 'training_set '+date+'.pkl'), \"rb\") as f:\n",
        "            train_data, train_labels = pkl.load(f)\n",
        "        with open(link_paths(actual_sets_location, 'testing_set '+date+'.pkl'), \"rb\") as f:\n",
        "            test_data, test_labels = pkl.load(f)\n",
        "        with open(link_paths(actual_sets_location, 'validation_set '+date+'.pkl'), \"rb\") as f:\n",
        "            val_data, val_labels = pkl.load(f)\n",
        "        with open(link_paths(actual_sets_location, 'settings '+date+'.pkl'), \"rb\") as f:\n",
        "            try:\n",
        "                settings = pkl.load(f)\n",
        "                # classes, n_classes, vocab_processor, len_vocabulary = pkl.load(f)\n",
        "            except:\n",
        "                settings = [None, None, None, None]\n",
        "        return train_data, test_data, val_data, train_labels, test_labels, val_labels, settings\n",
        "    except:\n",
        "        print('A problem occurred while loading the sets!')\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "def save_data_frame(script_key, data_frame, csvfile):\n",
        "    path_to_csv = get_root_location('model_dataframe/')\n",
        "    date = datetime.now().isoformat()\n",
        "    if csvfile:\n",
        "        output_path = fh.link_paths(path_to_csv, csvfile)\n",
        "    else:\n",
        "        output_path = fh.link_paths(path_to_csv, 'dataframe '+date+'.csv')\n",
        "    print('output_path: ', output_path)\n",
        "    \n",
        "    data_frame.to_csv(output_path, index=False, sep=',', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "\n",
        "def load_data_frame(script_key, csvfile):\n",
        "    path_to_csv = get_root_location('model_dataframe/')\n",
        "    if csvfile:\n",
        "        input_path = link_paths(path_to_csv, csvfile)\n",
        "    else:\n",
        "        input_path = link_paths(path_to_csv, 'dataframe '+date+'.csv')\n",
        "    print('input_path: ', input_path)\n",
        "    \n",
        "    data_frame = pd.read_csv(input_path, sep=',', quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \", header=None, engine='python')\n",
        "    data_frame.columns = ['patent_id', 'text', 'classification']\n",
        "    \n",
        "    # data_frame['text'] = data_frame['text'].apply(lambda text : clean_text(text))\n",
        "    data_frame = check_out_empty_texts_and_wrong_classcodes(data_frame)\n",
        "    \n",
        "    classification_df = pd.DataFrame(columns=['class', 'count'])\n",
        "    data_frame['classification'].apply(lambda classcode : calculate_class_distribution(classcode, classification_df))\n",
        "    return data_frame, classification_df\n",
        "\n",
        "def apply_simple_binarizer(classification, classes):\n",
        "    print('###  label_binarizer  ###')\n",
        "    binarized_classification = label_binarize(classification, classes=classes)\n",
        "    return binarized_classification, classes, binarized_classification.shape[1]\n",
        "\n",
        "# TODO: may be the same as the simple\n",
        "def apply_label_binarizer(classification):\n",
        "    lb = LabelBinarizer()\n",
        "    return lb.fit_transform(classification)\n",
        "\n",
        "def apply_multilabel_binarizer(data_frame):\n",
        "    print('###  multi_label_binarizer  ###')\n",
        "    ################################################ classification: from text to sparse binary matrix [[0, 1, 0],[1, 0, 1]]\n",
        "    temp_classification = data_frame.apply(lambda row : tokenize_complex_text_in_set(row['classification']), axis=1)\n",
        "    df_to_list = temp_classification.tolist()\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    mlb.fit(df_to_list)\n",
        "    classes = list(mlb.classes_)\n",
        "    return mlb.transform(df_to_list), classes, len(classes)\n",
        "\n",
        "def set_string_for_fasttext(item):\n",
        "    return str(item).replace(' or ', ' ').replace(', or ', ' ').replace(',', ' ').replace(',', '__label__').replace('$$', ' ').replace('$', ' ').replace(' ', ' __label__').replace('___', '__')\n",
        "\n",
        "def apply_keras_binarizer(classification, num_classes):\n",
        "    return keras.utils.to_categorical(classification, num_classes)\n",
        "\n",
        "def apply_tfidf_vectorizer_fit(data_frame):\n",
        "    print('###  tfidf_vectorizer_text  ###')\n",
        "    vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
        "    vectorizer.fit(data_frame['text'].apply(lambda x : np.str_(x)))\n",
        "    return vectorizer\n",
        "\n",
        "def apply_tfidf_vectorizer_transform(text, vectorizer):\n",
        "    return vectorizer.transform(text.apply(lambda x : np.str_(x)))\n",
        "\n",
        "def apply_word2vec_word_averaging(data_frame):\n",
        "    print('###  word2vec_word_averaging_vectorizer  ###')\n",
        "    # from gensim.models import Word2Vec\n",
        "    current_path = 'content/drive/My\\ Drive/Colab\\ Notebooks'\n",
        "    path_ = current_path[:current_path.rfind('/', 0, -8)] + \"/data/GoogleNews-vectors-negative300.bin\"\n",
        "    try:\n",
        "        wv = gensim.models.KeyedVectors.load_word2vec_format(path_, binary=True)\n",
        "    except:\n",
        "        print(\"unable to find word2vec model from google, downloading...\")\n",
        "        wv = gensim.models.KeyedVectors.load_word2vec_format(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "        print(\"done...\")\n",
        "    wv.init_sims(replace=True)\n",
        "    helper_word2vec = Word2VecHelper()\n",
        "\n",
        "    text_tokenized = data_frame.apply(lambda row : helper_word2vec.w2v_tokenize_text(row['text']), axis=1).values\n",
        "    return helper_word2vec.word_averaging_list(wv, text_tokenized)\n",
        "\n",
        "def apply_doc2vec(data_frame, type_):\n",
        "    Y, classes, n_classes = apply_classification_vectorizer(type_, data_frame)\n",
        "    X_train, X_test, y_train, y_test = get_train_test_from_data(data_frame, Y)\n",
        "    patent_ids = X_train['patent_id']\n",
        "\n",
        "    helper_doc2vec = Dov2VecHelper()\n",
        "\n",
        "    X_train = helper_doc2vec.label_sentences(X_train['text'], 'Train')\n",
        "    X_test = helper_doc2vec.label_sentences(X_test['text'], 'Test')\n",
        "    all_data = X_train + X_test\n",
        "\n",
        "    model_dbow = train_doc2vec(all_data)\n",
        "\n",
        "    train_vectors_dbow = helper_doc2vec.get_vectors(model_dbow, len(X_train), 150, 'Train')\n",
        "    test_vectors_dbow = helper_doc2vec.get_vectors(model_dbow, len(X_test), 150, 'Test')\n",
        "    return train_vectors_dbow, test_vectors_dbow,  y_train, y_test, classes, n_classes, patent_ids\n",
        "\n",
        "# here i have the ndarray.. again\n",
        "def apply_vocabulary_processor_new_tensorflow(text):\n",
        "    max_document_length = max([len(tokenize_text(x)) for x in text])\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<UNK>\")\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    text = tokenizer.texts_to_sequences(text)\n",
        "    text = tf.keras.preprocessing.sequence.pad_sequences(text, maxlen=max_document_length, padding='post', truncating='post')\n",
        "    return text, None, np.amax(text)\n",
        "\n",
        "def apply_vocabulary_processor(text):\n",
        "    from tensorflow.contrib import learn\n",
        "\n",
        "    max_document_length = max([len(tokenize_text(x)) for x in text])\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    return np.array(list(vocab_processor.fit_transform(text))), vocab_processor, len(vocab_processor.vocabulary_)\n",
        "\n",
        "def apply_count_vectorizer(data_frame):\n",
        "    # this should split every row as a instance - 8669 (both training and test)\n",
        "    # converts a set of strings to a set of integers\n",
        "    print('###  count_vectorizer  ###')\n",
        "    df_to_list = data_frame['text'].tolist()\n",
        "    vectorizer = CountVectorizer()\n",
        "    return vectorizer.fit_transform(df_to_list).toarray()\n",
        "\n",
        "def apply_standard_vectorizer(data_frame, type_):\n",
        "    Y, classes, n_classes = apply_classification_vectorizer(type_, data_frame)\n",
        "    try:\n",
        "        # X, vocab_processor, len_vocabulary = apply_vocabulary_processor(data_frame['text'])\n",
        "        X, vocab_processor, len_vocabulary = apply_vocabulary_processor_new_tensorflow(data_frame['text'])\n",
        "    except:\n",
        "        X = apply_count_vectorizer(data_frame)\n",
        "        vocab_processor, len_vocabulary = None, 0\n",
        "    # a list of the words used in the text, identified by a unique number for each different word\n",
        "\n",
        "    X = X.astype('float32')\n",
        "    X /= len_vocabulary\n",
        "\n",
        "    X_train, X_test, y_train, y_test = get_train_test_from_data(X, Y)\n",
        "\n",
        "    print(\"Vocabulary Size: {:d}\".format(len_vocabulary))\n",
        "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
        "    return X_train, X_test, y_train, y_test, classes, n_classes, vocab_processor, len_vocabulary\n",
        "\n",
        "def apply_classification_vectorizer(type_, data_frame):\n",
        "    classification = data_frame['classification']\n",
        "    if type_ == 'simple':\n",
        "        classes = ['H', 'B', 'C'] # useful if i need to shrink the set of classes\n",
        "        return apply_simple_binarizer(classification, classes)\n",
        "    elif type_ == 'label_binarizer':\n",
        "        return apply_label_binarizer(classification), None, 0\n",
        "    elif type_ == 'multi_label':\n",
        "        return apply_multilabel_binarizer(data_frame)\n",
        "    elif type_ == 'fasttext':\n",
        "        return ['__label__'+set_string_for_fasttext(item[:-1]) for item in data_frame['classification']], None, 0\n",
        "    elif type_ == 'keras':\n",
        "        # num_classes = 182\n",
        "        num_classes = 62\n",
        "        return apply_keras_binarizer(classification, num_classes)\n",
        "    return classification, None, 0\n",
        "\n",
        "def apply_df_vectorizer(data_frame, type_, classification_type, model_name):\n",
        "    # type_ = \"doc2vec\"\n",
        "    # classification_type = \"multi_label\"\n",
        "    \n",
        "    if type_ == 'doc2vec':\n",
        "        X_train, X_test, Y_train, Y_test, classes, n_classes, patent_ids = apply_doc2vec(data_frame, classification_type)\n",
        "        print('finisched apply doc2vec')\n",
        "        vocab_processor, len_vocabulary = None, 0\n",
        "    elif type_ == 'standard':\n",
        "        standard_results = apply_standard_vectorizer(data_frame, classification_type)\n",
        "        X_train, X_test, Y_train, Y_test, classes, n_classes, vocab_processor, len_vocabulary = standard_results\n",
        "        patent_ids = data_frame['patent_id']\n",
        "    else:\n",
        "        Y_vect, classes, n_classes = apply_classification_vectorizer(classification_type, data_frame)\n",
        "        vocab_processor, len_vocabulary = None, 0\n",
        "        if type_ == 'tfidf_fit_transform': # not sure it remains the same order\n",
        "            X_vect = pd.DataFrame({'text' : data_frame['text'], 'patent_id' : data_frame['patent_id']})\n",
        "            X_train, X_test, Y_train, Y_test = get_train_test_from_data(X_vect, Y_vect)\n",
        "\n",
        "            patent_ids = X_train['patent_id']\n",
        "\n",
        "            vectorizer = apply_tfidf_vectorizer_fit(data_frame)\n",
        "            X_train = apply_tfidf_vectorizer_transform(X_train['text'], vectorizer)\n",
        "            X_test = apply_tfidf_vectorizer_transform(X_test['text'], vectorizer)\n",
        "        elif type_ == 'word2vec': # required google pre-trained vectors, it starts downloading if you don't have\n",
        "            X_vect = apply_word2vec_word_averaging(data_frame)\n",
        "\n",
        "            text_indexes = data_frame.index.values\n",
        "\n",
        "            X_vect_temp = pd.DataFrame({'text' : text_indexes, 'patent_id' : data_frame['patent_id']})\n",
        "            X_train_temp, X_test_temp, Y_train, Y_test = get_train_test_from_data(X_vect_temp, Y_vect)\n",
        "\n",
        "            X_train, patent_ids = get_set_from_index(X_train_temp, X_vect)\n",
        "            X_test, _ = get_set_from_index(X_test_temp, X_vect)\n",
        "        else:\n",
        "            data_frame.drop(columns=['classification'])\n",
        "            X_train, X_test, Y_train, Y_test = get_train_test_from_data(data_frame, Y_vect)\n",
        "            return X_train, X_test, Y_train, Y_test, classes, n_classes, vocab_processor, len_vocabulary\n",
        "    save_training_set(patent_ids, model_name)\n",
        "    return X_train, X_test, Y_train, Y_test, classes, n_classes, vocab_processor, len_vocabulary\n",
        "\n",
        "def get_sequential_classifier_information(model):\n",
        "    text = str(model)\n",
        "    string_list = []\n",
        "    model.summary(print_fn=lambda x: string_list.append(x))\n",
        "    layers = ' '.join(string_list)\n",
        "\n",
        "    parameters = \"\"\n",
        "    for index, el in enumerate(layers.split(' (')):\n",
        "        if index > 0 and index < len(layers.split(' ('))-1:\n",
        "            if len(el.split(' ')[-1]) >= 5:\n",
        "                parameters += el.split(' ')[-1] + \" \"\n",
        "\n",
        "    return text.split(' ')[0].split('.')[-1], parameters\n",
        "\n",
        "def get_csv_path(model_key):\n",
        "    root_location = get_root_location('data/')\n",
        "    return link_paths(root_location, 'conv_results.csv')\n",
        "#     return 'content/drive/My\\ Drive/Colab\\ Notebooks/data/conv_results.csv'\n",
        "\n",
        "def apply_method_to_create_metrics(tuple_):\n",
        "    return list(tuple_) if tuple_ else 'None'\n",
        "\n",
        "def get_parameters_values(parameters):\n",
        "    parameters.replace(\" \", \"\")\n",
        "    return list(map(lambda token : token, parameters.split(' ')))\n",
        "\n",
        "def get_parameters_list(model, metrics, parameters, classif_approach, classif_level, classif_type, dataset_location):\n",
        "    list_ = [model]\n",
        "    list_ += list(map(lambda item : item, metrics))\n",
        "    list_ += [classif_approach, classif_level, classif_type]\n",
        "    list_ += list(map(lambda item : item, parameters))\n",
        "    list_ += [dataset_location]\n",
        "    return list_\n",
        "\n",
        "def get_metrics_values(metrics):\n",
        "    return list(map(lambda tuple_ : apply_method_to_create_metrics(tuple_), metrics))\n",
        "\n",
        "def get_sequential_LSTM_metrics_values(metrics):\n",
        "    micro = [\"precision:\"+str(metrics[\"precision_micro\"]), \"recall:\"+str(metrics[\"recall_micro\"]), \"f1:\"+str(metrics[\"f1_micro\"])]\n",
        "    macro = [\"precision:\"+str(metrics[\"precision_macro\"]), \"recall:\"+str(metrics[\"recall_macro\"]), \"f1:\"+str(metrics[\"f1_macro\"])]\n",
        "    tops = [\"top_1:\"+str(metrics[\"top_1\"]), \"top_3:\"+str(metrics[\"top_3\"]), \"top_5:\"+str(metrics[\"top_5\"])]\n",
        "    return [str(metrics[\"coverage_error\"]), micro, macro, tops]\n",
        "\n",
        "def get_sequential_layers_values(parameters):\n",
        "    return [list(map(lambda token : token, tokenize_text(parameters[:-1])))]\n",
        "\n",
        "def write_dataframe_as_csv(data_frame, path_to_csv):\n",
        "    if os.path.isfile(path_to_csv):\n",
        "        with open(path_to_csv, 'a') as f:\n",
        "            data_frame.to_csv(f, sep=',', header=False)\n",
        "    else:\n",
        "        data_frame.to_csv(path_to_csv, sep=',', header=True)\n",
        "\n",
        "settings = {\n",
        "    \"Sequential\" : {\n",
        "                            \"results_file_name\" : \"data/models_results/sequential_results.csv\",\n",
        "                            \"metrics\" : ['loss', 'accuracy', 'mse'],\n",
        "                            \"parameters\" : ['layers'],\n",
        "    },\n",
        "    \"Sequential Test\" : {\n",
        "                            \"results_file_name\" : \"data/models_results/sequential_results.csv\",\n",
        "                            \"metrics\" : ['loss', 'accuracy', 'mse', 'micro', 'macro'],\n",
        "                            \"parameters\" : ['layers'],\n",
        "    },\n",
        "    \"Sequential Val\" : {\n",
        "                            \"results_file_name\" : \"data/models_results/sequential_results.csv\",\n",
        "                            \"metrics\" : ['loss', 'accuracy', 'mse', 'micro', 'macro'],\n",
        "                            \"parameters\" : ['layers'],\n",
        "    }\n",
        "}\n",
        "\n",
        "def get_saving_dataframe(model_key, metrics_values, parameters_values, classif_approach, classif_level, classif_type, dataset_location):\n",
        "    columns_list = get_parameters_list('model_name', settings[model_key][\"metrics\"], settings[model_key][\"parameters\"], 'approach', 'level', 'type', 'dataset_location')\n",
        "    values_list = get_parameters_list(model_key, metrics_values, parameters_values, classif_approach, classif_level, classif_type, dataset_location)\n",
        "    data_frame = pd.DataFrame(columns=columns_list)\n",
        "    data_frame.loc[data_frame.shape[0] + 1] = values_list\n",
        "    return data_frame\n",
        "\n",
        "def save_results_function(model_key, metrics, parameters, classif_approach, classif_level, classif_type, dataset_location):\n",
        "    print('###  saving_results  ###')\n",
        "    path_to_csv = get_csv_path(model_key)\n",
        "\n",
        "    if model_key == 'Sequential':\n",
        "        metrics = list(map(lambda metric : [metric], metrics))\n",
        "        # metrics = [[metric] for metric in metrics]\n",
        "        metrics_values = get_metrics_values(metrics)\n",
        "        parameters_values = get_sequential_layers_values(parameters)\n",
        "    elif model_key == 'Sequential Test' or 'Sequential Val':\n",
        "        metrics = [[-1], [-1], [-1], # metrics['loss'], metrics['accuracy'], metrics['mse'],\n",
        "                   [metrics['precision_micro'], metrics['recall_micro'], metrics['f1_micro']],\n",
        "                   [metrics['precision_macro'], metrics['recall_macro'], metrics['f1_macro']]]\n",
        "        metrics_values = get_metrics_values(metrics)\n",
        "        parameters_values = get_sequential_layers_values(parameters)\n",
        "    elif model_key == 'FastText':\n",
        "        metrics_values = metrics[1:3]\n",
        "        parameters_values = [parameters]\n",
        "    elif model_key == 'Sequential_LSTM':\n",
        "        temp_metrics = metrics\n",
        "        del temp_metrics[\"total_positive\"], temp_metrics[\"average_num_of_labels\"]\n",
        "        metrics_values = get_sequential_LSTM_metrics_values(temp_metrics)\n",
        "        parameters_values = get_sequential_layers_values(parameters)\n",
        "    else:\n",
        "        metrics_values = get_metrics_values(metrics)\n",
        "        parameters_values = get_parameters_values(parameters)\n",
        "\n",
        "    data_frame = get_saving_dataframe(model_key, metrics_values, parameters_values, classif_approach, classif_level, classif_type, dataset_location)\n",
        "    write_dataframe_as_csv(data_frame, path_to_csv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gh2TeplRfMox",
        "colab": {}
      },
      "source": [
        "# word model helper\n",
        "class Word2VecHelper():\n",
        "    def __init__(self):\n",
        "        print('###  word2vec_vectorizer  ###')\n",
        "        self.tokens = []\n",
        "        self.mean = []\n",
        "\n",
        "    def w2v_tokenize_text(self, text):\n",
        "        self.tokens = []\n",
        "        for word in th.tokenize_text(text):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            self.tokens.append(word)\n",
        "        return self.tokens\n",
        "\n",
        "    def word_averaging(self, wv, words):\n",
        "        all_words = set()\n",
        "        self.mean = []\n",
        "\n",
        "        for word in words:\n",
        "            if isinstance(word, np.ndarray):\n",
        "                self.mean.append(word)\n",
        "            elif word in wv.vocab:\n",
        "                self.mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "                all_words.add(wv.vocab[word].index)\n",
        "\n",
        "        if not self.mean:\n",
        "            print(\"POSSIBLE ERROR IN WORD2VECHELPER: cannot compute similarity with no input %s\", words)\n",
        "            return np.zeros(wv.vector_size,)\n",
        "\n",
        "        self.mean = gensim.matutils.unitvec(np.array(self.mean).mean(axis=0)).astype(np.float32)\n",
        "        return self.mean\n",
        "\n",
        "    def  word_averaging_list(self, wv, text_list):\n",
        "        return np.vstack([self.word_averaging(wv, post) for post in text_list])\n",
        "    \n",
        "class Dov2VecHelper():\n",
        "    def __init__(self):\n",
        "        print('###  doc2vec_vectorizer  ###')\n",
        "        self.labeled = []\n",
        "        self.vectors = []\n",
        "\n",
        "    def label_sentences(self, corpus, label_type):\n",
        "        \"\"\"\n",
        "        Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
        "        We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
        "        a dummy index of the post.\n",
        "        \"\"\"\n",
        "        self.labeled = []\n",
        "        for i, v in enumerate(corpus):\n",
        "            label = label_type + '_' + str(i)\n",
        "            self.labeled.append(TaggedDocument(tokenize_text(v), [label]))\n",
        "        return self.labeled\n",
        "\n",
        "    def get_vectors(self, model, corpus_size, vectors_size, vectors_type):\n",
        "        \"\"\"\n",
        "        Get vectors from trained doc2vec model\n",
        "        :param doc2vec_model: Trained Doc2Vec model\n",
        "        :param corpus_size: Size of the data\n",
        "        :param vectors_size: Size of the embedding vectors\n",
        "        :param vectors_type: Training or Testing vectors\n",
        "        :return: list of vectors\n",
        "        \"\"\"\n",
        "        self.vectors = np.zeros((corpus_size, vectors_size))\n",
        "        for i in range(0, corpus_size):\n",
        "            prefix = vectors_type + '_' + str(i)\n",
        "            self.vectors[i] = model.docvecs[prefix]\n",
        "        return self.vectors\n",
        "\n",
        "def utils_shuffle_rows(list_):\n",
        "    return utils.shuffle(list_)\n",
        "\n",
        "def train_doc2vec(data_):\n",
        "    cores = multiprocessing.cpu_count()\n",
        "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065, sample=0, workers=cores)\n",
        "    model_dbow.build_vocab([x for x in tqdm(data_)])\n",
        "\n",
        "    for epoch in range(30):\n",
        "        model_dbow.train(utils_shuffle_rows([x for x in tqdm(data_)]), total_examples=len(data_), epochs=1)\n",
        "        model_dbow.alpha -= 0.002\n",
        "        model_dbow.min_alpha = model_dbow.alpha\n",
        "    return model_dbow\n",
        "\n",
        "def get_word2vec_model(data_):\n",
        "    # let X be a list of tokenized texts (i.e. list of lists of tokens)\n",
        "    model = Word2Vec(data_, size=100)\n",
        "    return dict(zip(model.wv.index2word, model.wv.syn0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JmHQsEcEW5NH",
        "colab": {}
      },
      "source": [
        "# data helper\n",
        "def fill_matrix(data_matrix, source_dict, docs_list):\n",
        "    \"\"\"\n",
        "    the use_get flag is for doc2vec_model.docvecs since it doesnt support .get(), so we catch the exception and\n",
        "    fill with zeros in that case. This should really happen very rarely (if ever) so this exception handling\n",
        "    should not be a drain on performance\n",
        "    \"\"\"\n",
        "    for i, doc_id in enumerate(docs_list):\n",
        "        child_ids = doc_id\n",
        "        j = 0\n",
        "        try:\n",
        "            if source_dict[child_ids] is not None:\n",
        "                data_matrix[i][j] = source_dict[child_ids]\n",
        "            else:\n",
        "                print('here')\n",
        "                data_matrix[i][j] = [0]\n",
        "        except:\n",
        "            print('{} not seen in doc2vec model'.format(doc_id))\n",
        "            data_matrix[i][j] = [0]\n",
        "\n",
        "def get_single_df(docs_list, sequence_size, embedding_size, doc2vec_docvecs):\n",
        "    data_ = np.ndarray((len(docs_list), sequence_size, embedding_size), dtype=np.float32)\n",
        "    fill_matrix(data_, doc2vec_docvecs, docs_list)\n",
        "    return data_\n",
        "\n",
        "def get_df_data(num_data, training_docs_list, val_docs_list, test_docs_list, sequence_size, embedding_size, doc2vec_model_location):\n",
        "    doc2vec_model = get_doc2vec_model(doc2vec_model_location)\n",
        "    if num_data == 2:\n",
        "        X_data = get_single_df(training_docs_list, sequence_size, embedding_size, doc2vec_model.docvecs)\n",
        "        Xv_data = get_single_df(val_docs_list, sequence_size, embedding_size, doc2vec_model.docvecs)\n",
        "        Xt_data = None\n",
        "    elif num_data == 3:\n",
        "        X_data = get_single_df(training_docs_list, sequence_size, embedding_size, doc2vec_model.docvecs)\n",
        "        Xv_data = get_single_df(val_docs_list, sequence_size, embedding_size, doc2vec_model.docvecs)\n",
        "        Xt_data = get_single_df(test_docs_list, sequence_size, embedding_size, doc2vec_model.docvecs)\n",
        "    return X_data, Xv_data, Xt_data\n",
        "\n",
        "def batch_generator(input_file, label_file, batch_size, queue_size, is_mlp=False, validate=False):\n",
        "    q = Queue(maxsize=queue_size)\n",
        "    p = ArrayReader(input_file, label_file, q, batch_size, is_mlp, validate)\n",
        "    p.start()\n",
        "    while True:\n",
        "        item = q.get()\n",
        "        if not item:\n",
        "            p.terminate()\n",
        "            print('Finished batch iteration')\n",
        "            raise StopIteration()\n",
        "        else:\n",
        "            yield item\n",
        "\n",
        "class ArrayReader(Process):\n",
        "    def __init__(self, input_file, label_file, out_queue, batch_size, is_mlp=False, validate=False):\n",
        "        super(ArrayReader, self).__init__()\n",
        "        self.is_mlp = is_mlp\n",
        "        self.validate = validate\n",
        "        self.q = out_queue\n",
        "        self.batch_size = batch_size\n",
        "        self.input_file = input_file\n",
        "        self.label_file = label_file\n",
        "\n",
        "    def run(self):\n",
        "        # x_file = np.load(self.input_file, mmap_mode='r')\n",
        "        # y_file = np.load(self.label_file, mmap_mode='r')\n",
        "        x_file = self.input_file\n",
        "        y_file = self.label_file\n",
        "        start_item = 0\n",
        "        num_iter = 0\n",
        "        while True:\n",
        "            \n",
        "            if start_item > y_file.shape[0]:\n",
        "                # print('in new epoch for {}'.format(os.path.basename(self.input_file)))\n",
        "                print('\\nin new epoch for {}'.format(os.path.basename('X_data, y_train with validation data')))\n",
        "                start_item = 0\n",
        "            y_batch = y_file[start_item: start_item + self.batch_size]\n",
        "            x_batch = x_file[start_item: start_item + self.batch_size]\n",
        "            if self.is_mlp:\n",
        "                x_batch = np.reshape(x_batch, (x_batch.shape[0], x_batch.shape[1] * x_batch.shape[2]))\n",
        "            start_item += self.batch_size\n",
        "            num_iter += 1\n",
        "            try:\n",
        "                self.q.put((x_batch, y_batch), block=True)\n",
        "            except:\n",
        "                return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qjJAhZ6ylMhd",
        "colab": {}
      },
      "source": [
        "# metrics helper\n",
        "# get_binary_0_5 = lambda x: 1 if x > 0.40 else 0 # sectiors\n",
        "get_binary_0_5 = lambda x: 1 if x > 0.05 else 0 # subclasses\n",
        "get_binary_0_5 = np.vectorize(get_binary_0_5)\n",
        "\n",
        "metrics_graph_ranges = {\n",
        "    'sections': {'min':0, 'max': 0.5},\n",
        "    'classes': {'min':0, 'max': 0.05},\n",
        "    'subclasses': {'min':0, 'max': 0.05}\n",
        "}\n",
        "\n",
        "def get_binary_classification(predictions, threshold):\n",
        "    binary_predictions = lambda prediction : 1 if prediction > threshold else 0\n",
        "    return np.vectorize(binary_predictions)\n",
        "\n",
        "def display_sequential_metrics(algorithm, metrics):\n",
        "    print('###  calculating_metrics  ###')\n",
        "    print('@@@  ' + algorithm + '  @@@')\n",
        "    print(\"Over all labels - Coverage error: {:.3f}, Average labels: {:.3f}\".format(\n",
        "        metrics['coverage_error'], metrics['average_num_of_labels']))\n",
        "    print(\"Percentage - Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}\".format(\n",
        "        metrics['top_1'], metrics['top_3'], metrics['top_5']))\n",
        "    print(\"Accuracy: {:.3f}\".format(metrics['accuracy']))\n",
        "    print(\"Macro - precision: {:.3f}, recall: {:.3f}, f1: {:.3f}\".format(\n",
        "        metrics['precision_macro'], metrics['recall_macro'], metrics['f1_macro']))\n",
        "    print(\"Micro - precision: {:.3f}, recall: {:.3f}, f1: {:.3f}\".format(\n",
        "        metrics['precision_micro'], metrics['recall_micro'], metrics['f1_micro']))\n",
        "\n",
        "def get_top_N_percentage(y_score, y_true, max_N=3):\n",
        "    \"\"\"\n",
        "    Get percentage of correct labels that are in the top N scores\n",
        "    \"\"\"\n",
        "    num_all_true = 0\n",
        "    num_found_in_max_N = 0\n",
        "    for i in range(y_score.shape[0]):\n",
        "        y_score_row = y_score[i, :]\n",
        "        y_true_row = y_true[i, :]\n",
        "        desc_score_indices = np.argsort(y_score_row)[::-1]\n",
        "        true_indices = np.where(y_true_row ==1)[0]\n",
        "\n",
        "        num_true_in_row = len(true_indices)\n",
        "        num_all_true += num_true_in_row\n",
        "        for i, score_index in enumerate(desc_score_indices):\n",
        "            # only iterate through the score list till depth N, but make sure you also account for the case where\n",
        "            # the number of true labels for the current row is higher than N\n",
        "            if i >= max_N and i >= num_true_in_row:\n",
        "                break\n",
        "            if score_index in true_indices:\n",
        "                num_found_in_max_N += 1\n",
        "    return float(num_found_in_max_N) / num_all_true\n",
        "\n",
        "def get_sequential_metrics(y_true, y_pred, y_binary_pred):\n",
        "    \"\"\"\n",
        "    create the metrics object containing all relevant metrics\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        if y_true.shape[1] > 1 or y_binary_pred.shape[1] > 1:\n",
        "            y_true_unrolled = y_true.ravel()\n",
        "            y_binary_pred_unrolled = y_binary_pred.ravel()\n",
        "            y_true_unrolled = y_true\n",
        "            y_binary_pred_unrolled = y_binary_pred\n",
        "    except:\n",
        "        print('exception in calculate metrics - line 110 metrics_helper.py')\n",
        "        pass\n",
        "\n",
        "    metrics = {}\n",
        "    metrics['total_positive'] = np.sum(np.sum(y_binary_pred))\n",
        "\n",
        "    metrics['y_true'] = y_true\n",
        "    metrics['y_pred'] = y_pred\n",
        "    metrics['y_binary_pred'] = y_binary_pred\n",
        "  \n",
        "    metrics['coverage_error'] = coverage_error(y_true, y_pred)\n",
        "    metrics['average_num_of_labels'] = round(float(np.sum(np.sum(y_true, axis=1))) / y_true.shape[0], 2)\n",
        "\n",
        "    metrics['average_precision_micro'] = average_precision_score(y_true_unrolled, y_binary_pred_unrolled, average='micro')\n",
        "    metrics['average_precision_macro'] = average_precision_score(y_true_unrolled, y_binary_pred_unrolled, average='macro')\n",
        "\n",
        "    metrics['accuracy'] = accuracy_score(y_true_unrolled, y_binary_pred_unrolled)\n",
        "\n",
        "    metrics['precision_micro'] = precision_score(y_true_unrolled, y_binary_pred_unrolled, average='micro')\n",
        "    metrics['precision_macro'] = precision_score(y_true_unrolled, y_binary_pred_unrolled, average='macro')\n",
        "    metrics['recall_micro'] = recall_score(y_true_unrolled, y_binary_pred_unrolled, average='micro')\n",
        "    metrics['recall_macro'] = recall_score(y_true_unrolled, y_binary_pred_unrolled, average='macro')\n",
        "    metrics['f1_micro'] = f1_score(y_true_unrolled, y_binary_pred_unrolled, average='micro')\n",
        "    metrics['f1_macro'] = f1_score(y_true_unrolled, y_binary_pred_unrolled, average='macro')\n",
        "\n",
        "    # only calculate those for cases with a small number of labels (sections only)\n",
        "    if y_true.shape[1] < 100:\n",
        "        precision_scores = np.zeros(y_true.shape[1])\n",
        "        for i in range(0, y_true.shape[1]):\n",
        "            precision_scores[i] = precision_score(y_true[:,i], y_binary_pred[:,i])\n",
        "        metrics['precision_scores_array'] = precision_scores.tolist()\n",
        "\n",
        "        recall_scores = np.zeros(y_true.shape[1])\n",
        "        for i in range(0, y_true.shape[1]):\n",
        "            recall_scores[i] = recall_score(y_true[:,i], y_binary_pred[:,i])\n",
        "        metrics['recall_scores_array'] = recall_scores.tolist()\n",
        "\n",
        "        f1_scores = np.zeros(y_true.shape[1])\n",
        "        for i in range(0, y_true.shape[1]):\n",
        "            f1_scores[i] = f1_score(y_true[:,i], y_binary_pred[:,i])\n",
        "        metrics['f1_scores_array'] = f1_scores.tolist()\n",
        "\n",
        "    metrics['top_1'] = get_top_N_percentage(y_pred, y_true, max_N=1)\n",
        "    metrics['top_3'] = get_top_N_percentage(y_pred, y_true, max_N=3)\n",
        "    metrics['top_5'] = get_top_N_percentage(y_pred, y_true, max_N=5)\n",
        "    return metrics\n",
        "\n",
        "def calculate_manual_metrics(algorithm, y_true, y_pred):\n",
        "    a, b = y_true.T.copy(order = 'C'), y_pred.T.copy(order = 'C')\n",
        "    \n",
        "    print('//////// \\ntrue_values:')\n",
        "    print(a)\n",
        "    print(a.shape)\n",
        "    print('//////// \\npredictions:')\n",
        "    print(b)\n",
        "    print(b.shape)\n",
        "\n",
        "    count = 0\n",
        "    count_tp, count_fp, count_tn, count_fn = 0, 0, 0, 0\n",
        "    temp_count_tp, temp_count_fp, temp_count_tn, temp_count_fn = 0, 0, 0, 0\n",
        "    temp_precision, temp_recall = [], []\n",
        "    for y_test_el, y_pred_el in zip(np.nditer(a), np.nditer(b)):\n",
        "        # print(y_test_el, y_pred_el, end=', ')\n",
        "        count += 1\n",
        "        if y_pred_el == 1 and y_test_el == 1:\n",
        "            temp_count_tp += 1\n",
        "        if y_pred_el == 1 and y_test_el == 0:\n",
        "            temp_count_fp += 1\n",
        "        if y_pred_el == 0 and y_test_el == 0:\n",
        "            temp_count_tn += 1\n",
        "        if y_pred_el == 0 and y_test_el == 1:\n",
        "            temp_count_fn += 1\n",
        "        if count % y_true.shape[0] == 0:\n",
        "            # print(' ', count)\n",
        "            if temp_count_tp+temp_count_fp != 0:\n",
        "                precision = temp_count_tp/(temp_count_tp+temp_count_fp)\n",
        "            else:\n",
        "                precision = 0\n",
        "            if temp_count_tp+temp_count_fn != 0:\n",
        "                recall = temp_count_tp/(temp_count_tp+temp_count_fn)\n",
        "            else:\n",
        "                recall = 0\n",
        "            temp_precision.append(precision)\n",
        "            temp_recall.append(recall)\n",
        "\n",
        "            count_tp += temp_count_tp\n",
        "            count_fp += temp_count_fp\n",
        "            count_tn += temp_count_tn\n",
        "            count_fn += temp_count_fn\n",
        "\n",
        "            temp_count_tp, temp_count_fp, temp_count_tn, temp_count_fn = 0, 0, 0, 0\n",
        "\n",
        "    print('\\nglobal - true positives : ', count_tp, 'true negatives : ', count_tn, 'false positives : ', count_fp, 'false negatives : ', count_fn)\n",
        "    print('per class - precision values to average: ', temp_precision)\n",
        "    print('per class - recall values to average: ', temp_recall)\n",
        "\n",
        "    accuracy = (count_tp+count_tn)/(count_tp+count_tn+count_fp+count_fn)\n",
        "    try:\n",
        "        precision = count_tp/(count_tp+count_fp)\n",
        "    except:\n",
        "        precision = 0\n",
        "    try:\n",
        "        recall = count_tp/(count_tp+count_fn)\n",
        "    except:\n",
        "        recall = 0\n",
        "    try:\n",
        "        f1_score = 2*(precision*recall)/(precision+recall)\n",
        "    except:\n",
        "        f1_score = 0\n",
        "\n",
        "    average_precision = sum(temp_precision)/len(temp_precision)\n",
        "    average_recall = sum(temp_recall)/len(temp_recall)\n",
        "    average_f1_score = 2*(average_precision*average_recall)/(average_precision+average_recall)\n",
        "\n",
        "    print('test accuracy : {0:0.5f}'.format(accuracy))\n",
        "    print('micro test precision : {0:0.5f},'.format(precision), ' macro test precision : {0:0.5f}'.format(average_precision))\n",
        "    print('micro test recall : {0:0.5f},'.format(recall), ' macro test recall : {0:0.5f}'.format(average_recall))\n",
        "    print('micro test f1_score : {0:0.5f},'.format(f1_score), ' macro test f1_score : {0:0.5f}'.format(average_f1_score))\n",
        "    return [-1,-1,-1,-1], [-1,-1,-1,-1], [accuracy, precision, recall, f1_score], [accuracy, average_precision, average_recall, average_f1_score]\n",
        "\n",
        "class MetricsCNNCallback(Callback):\n",
        "    \"\"\"\n",
        "    Callback called by keras after each epoch. Records the best validation loss and periodically checks the\n",
        "    validation metrics\n",
        "    \"\"\"\n",
        "    def __init__(self, val_data, val_labels, patience):\n",
        "        super(MetricsCNNCallback, self).__init__()\n",
        "        self.val_data = val_data\n",
        "        self.val_labels = val_labels\n",
        "\n",
        "        self.patience = patience\n",
        "\n",
        "        self.best_val_loss = None\n",
        "        self.best_weights = None\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.epoch_index = 0\n",
        "\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "        self.metrics_dict = {}\n",
        "        self.best_val_loss = np.Inf\n",
        "        self.best_weights = np.Inf\n",
        "        self.best_validation_metrics = None\n",
        "\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "        self.val_predictions = None\n",
        "        self.binary_predictions = None\n",
        "        self.val_loss, self.val_acc, self.val_mse = 0, 0, 0\n",
        "        self.validation_metrics = {}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.epoch_index += 1\n",
        "\n",
        "        actual_val_loss = logs.get('val_loss')\n",
        "        loss = logs.get('loss')\n",
        "        self.losses.append(loss)\n",
        "        self.val_losses.append(actual_val_loss)\n",
        "\n",
        "        if np.less(actual_val_loss, self.best_val_loss):\n",
        "            self.wait = 0\n",
        "            self.best_val_loss = actual_val_loss\n",
        "            self.best_weights = self.model.get_weights()\n",
        "            print('Found lower val loss for epoch {} => {}'.format(self.epoch_index, round(actual_val_loss, 5)))\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if np.less(self.patience, self.wait):\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "                print('Restoring model weights from the end of the best epoch.')\n",
        "                self.model.set_weights(self.best_weights)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
        "\n",
        "class LossHistory(Callback):\n",
        "    def __init__(self, params, losses_path):\n",
        "        self.params = params\n",
        "        self.losses_path = losses_path\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.lr = []\n",
        " \n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.lr.append(step_decay(len(self.losses)))\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        with open(self.losses_path, mode='w') as csv_file:\n",
        "            csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "            csv_writer.writerow([json.dumps(self.params), self.losses])\n",
        "\n",
        "# lr = lr0 * drop^floor(epoch / epochs_drop) \n",
        "def step_decay(epoch):\n",
        "   initial_lrate = 0.1\n",
        "   drop = 0.5\n",
        "   epochs_drop = 10.0\n",
        "   lrate = initial_lrate * math.pow(drop,  \n",
        "           math.floor((1+epoch)/epochs_drop))\n",
        "   return lrate\n",
        "\n",
        "# lr = lr0 * e^(−kt)\n",
        "def exp_decay(epoch):\n",
        "   initial_lrate = 0.1\n",
        "   k = 0.1\n",
        "   lrate = initial_lrate * exp(-k*t)\n",
        "   return lrate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DTzsDzdWxFLT",
        "colab": {}
      },
      "source": [
        "# doc2vec sequential helper\n",
        "def get_doc2vec_model(model_path):\n",
        "    return Doc2Vec.load(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VhWGOItRoUn_",
        "colab": {}
      },
      "source": [
        "# predicting model helper\n",
        "def plot_charts(history):\n",
        "    from matplotlib import pyplot\n",
        "\n",
        "    # plot loss during training\n",
        "    pyplot.subplot(211)\n",
        "    pyplot.title('Loss')\n",
        "    pyplot.plot(history.history['loss'], label='train')\n",
        "    pyplot.plot(history.history['val_loss'], label='test')\n",
        "    pyplot.legend()\n",
        "\n",
        "    # plot accuracy during training\n",
        "    pyplot.subplot(212)\n",
        "    pyplot.title('Accuracy')\n",
        "    pyplot.plot(history.history['accuracy'], label='train')\n",
        "    pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "    pyplot.legend()\n",
        "    pyplot.show()\n",
        "\n",
        "def get_cnn(optimizer, \n",
        "            init_mode_1, activation_1,\n",
        "            init_mode_2, activation_2,\n",
        "            init_mode_3, activation_3,\n",
        "            init_mode_4, activation_4,\n",
        "            init_mode_5, activation_5,\n",
        "            init_mode_6, activation_6,\n",
        "            weight_constraint_1, \n",
        "            weight_constraint_2, \n",
        "            weight_constraint_3, \n",
        "            weight_constraint_4, \n",
        "            weight_constraint_5, \n",
        "            weight_constraint_6, \n",
        "            dropout_rate_1, \n",
        "            dropout_rate_2, \n",
        "            dropout_rate_3, \n",
        "            dropout_rate_4,\n",
        "            neurons_1, neurons_2, neurons_3,\n",
        "            filters_1, filters_2, filters_3,\n",
        "            kernel_size_1, kernel_size_2, kernel_size_3,\n",
        "            strides_1, strides_2, strides_3, \n",
        "            len_vocabulary, input_size, n_classes):\n",
        "    # build up the model\n",
        "\n",
        "    # set up the layers\n",
        "    model = keras.Sequential()\n",
        "    # first option: with embedding layer\n",
        "    # model.add(keras.layers.Flatten(input_shape=(1, input_size))) # for doc2vec data\n",
        "    # model.add(keras.layers.Embedding(input_dim=input_size, output_dim=neurons_1))\n",
        "    \n",
        "    # second option: without embedding layer and flatten\n",
        "    model.add(keras.layers.ZeroPadding1D((1,1), input_shape=(1, input_size)))\n",
        "    \n",
        "    model.add(keras.layers.Conv1D(filters=filters_1, kernel_size=kernel_size_1, strides=strides_1, activation=activation_1, \n",
        "                                  kernel_initializer=init_mode_1, kernel_constraint=tf.keras.constraints.max_norm(weight_constraint_1)))\n",
        "    model.add(keras.layers.MaxPooling1D())\n",
        "\n",
        "    # model.add(keras.layers.Conv1D(filters=filters_1, input_shape=(1, input_size), kernel_size=kernel_size_1, strides=strides_1, activation=activation_1, \n",
        "    #                               kernel_initializer=init_mode_1, kernel_constraint=tf.keras.constraints.max_norm(weight_constraint_1)))\n",
        "\n",
        "    # model.add(keras.layers.ZeroPadding1D((1,1), input_shape=(len_vocabulary, neurons_1)))\n",
        "    # model.add(keras.layers.Conv1D(filters=filters_2, kernel_size=kernel_size_2, strides=strides_2, activation=activation_2, \n",
        "    #                               kernel_initializer=init_mode_2, kernel_constraint=tf.keras.constraints.max_norm(weight_constraint_2)))\n",
        "    # model.add(keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "    # model.add(keras.layers.ZeroPadding1D((1,1), input_shape=(len_vocabulary, neurons_1)))\n",
        "    # model.add(keras.layers.Conv1D(filters=filters_3, kernel_size=kernel_size_3, strides=strides_3, activation=activation_3, kernel_initializer=init_mode_3, kernel_constraint=max_norm(weight_constraint_3)))\n",
        "    # model.add(keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "    model.add(keras.layers.Flatten()) # commenting this avoids None problem in dense layer, but i need it for shrinking the tokens into a document without(64, 10..., 636) -> with(64, 636)\n",
        "    # model.add(keras.layers.Dropout(dropout_rate_1))\n",
        "    model.add(keras.layers.Dense(neurons_1, activation=activation_4, kernel_initializer=init_mode_4, \n",
        "                                 kernel_constraint=tf.keras.constraints.max_norm(weight_constraint_4)))\n",
        "    # model.add(keras.layers.Dropout(dropout_rate_2))\n",
        "    model.add(keras.layers.Dense(neurons_2, activation=activation_5, kernel_initializer=init_mode_5, \n",
        "                                 kernel_constraint=tf.keras.constraints.max_norm(weight_constraint_5)))\n",
        "    # model.add(keras.layers.Dropout(dropout_rate_3))\n",
        "    model.add(keras.layers.Dense(neurons_3, activation=activation_6, kernel_initializer=init_mode_6, \n",
        "                                 kernel_constraint=tf.keras.constraints.max_norm(weight_constraint_6)))\n",
        "    # model.add(keras.layers.Dropout(dropout_rate_4))\n",
        "    model.add(keras.layers.Dense(n_classes, activation=tf.nn.sigmoid))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # if optimizer == 'Adam':\n",
        "    #     optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "    model.compile(optimizer=optimizer, \n",
        "                  # loss='binary_crossentropy', \n",
        "                  loss='categorical_crossentropy', # multiclass\n",
        "                  metrics=['accuracy', 'mse'])\n",
        "    return model\n",
        "\n",
        "def run_cnn(model, \n",
        "            train_data, train_labels, test_data, test_labels, val_data, val_labels, \n",
        "            batch_size, callbacks_list,\n",
        "            train_flag,\n",
        "            queue_size = 100,\n",
        "            predicting_batch_size = 64, predicting_steps = None, predicting_max_queue_size = 100, predicting_callbacks_list = None, \n",
        "            epoch = 400):\n",
        "    print('###  CNN  ###')\n",
        "    # compile the model\n",
        "\n",
        "    # train the model\n",
        "    # history = model.fit(train_data,\n",
        "    #                     train_labels,\n",
        "    #                     epochs=epoch,\n",
        "    #                     batch_size=batch_size,\n",
        "    #                     validation_data=(val_data, val_labels),\n",
        "    #                     callbacks=callbacks_list,\n",
        "    #                     verbose=1)\n",
        "    print(train_data.shape)\n",
        "    print(test_data.shape)\n",
        "    print(val_data.shape)\n",
        "    print(len(val_data), batch_size, len(val_data) // batch_size)\n",
        "    if train_flag:\n",
        "        history = model.fit_generator(generator=batch_generator(train_data, train_labels, batch_size, queue_size, is_mlp=False, validate=False),\n",
        "                                      validation_data=batch_generator(val_data, val_labels, batch_size, queue_size, is_mlp=False, validate=True),\n",
        "                                      steps_per_epoch=len(train_data) // batch_size,\n",
        "                                      # samples_per_epoch=len(train_data) // batch_size,\n",
        "                                      validation_steps=len(val_data) // batch_size if (len(val_data) // batch_size) != 0 else 1,\n",
        "                                      # val_samples=len(val_data) // batch_size,\n",
        "                                      # nb_val_samples=len(val_data),\n",
        "                                      epochs=epoch,\n",
        "                                      callbacks=callbacks_list,\n",
        "                                      max_queue_size=queue_size)\n",
        "        # plot_charts(history)\n",
        "        print('finished training')\n",
        "    # val_loss, val_acc, val_mse = model.evaluate(val_data, val_labels)\n",
        "    # check on the test dataset\n",
        "    # test_loss, test_acc, test_mse = model.evaluate(test_data, test_labels)\n",
        "\n",
        "    # val_loss, val_acc, val_mse = model.evaluate_generator(generator=batch_generator(val_data, val_labels, predicting_batch_size, predicting_max_queue_size, is_mlp=False, validate=True),\n",
        "    #                                                       max_queue_size=predicting_max_queue_size,\n",
        "    #                                                       steps=len(val_data) // batch_size)\n",
        "\n",
        "    test_loss, test_acc, test_mse = model.evaluate_generator(generator=batch_generator(test_data, test_labels, predicting_batch_size, predicting_max_queue_size, is_mlp=False, validate=True),\n",
        "                                                             max_queue_size=predicting_max_queue_size,\n",
        "                                                             steps=len(test_data) // batch_size if (len(test_data) // batch_size) != 0 else 1)\n",
        "\n",
        "    # predictions = model.predict(test_data,\n",
        "    #                             batch_size=predicting_batch_size,\n",
        "    #                             steps=predicting_steps,\n",
        "    #                             max_queue_size=predicting_max_queue_size,\n",
        "    #                             callbacks=predicting_callbacks_list,\n",
        "    #                             verbose=1)\n",
        "\n",
        "    predictions = model.predict_generator(generator=batch_generator(test_data, test_labels, predicting_batch_size, predicting_max_queue_size, is_mlp=False, validate=True),\n",
        "                                          max_queue_size=predicting_max_queue_size,\n",
        "                                          steps=len(test_data) // batch_size if (len(test_data) // batch_size) != 0 else 1)\n",
        "    pred_classes = model.predict_classes(test_data, verbose=0)\n",
        "\n",
        "    print('finished predicting')\n",
        "\n",
        "    print('output print: ', predictions.shape, len(val_data), batch_size, len(val_data) // batch_size)\n",
        "    # return [test_loss, test_acc, test_mse], predictions, [val_loss, val_acc, val_mse]\n",
        "    return [test_loss, test_acc, test_mse], predictions, pred_classes, [None, None, None]\n",
        "    # return [None, None, None], predictions, pred_classes, [None, None, None]\n",
        "\n",
        "def save_sequential_model(model, model_path):\n",
        "    model.save(model_path)\n",
        "    \n",
        "def load_sequential_model(model_path):\n",
        "    return load_model(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ChVDq1VQYZxj",
        "colab": {}
      },
      "source": [
        "script_key = \"convolutional\"\n",
        "\n",
        "# def load_data_and_labels(positive_data_file, negative_data_file):\n",
        "#     \"\"\"\n",
        "#     Loads MR polarity data from files, splits the data into words and generates labels.\n",
        "#     Returns split sentences and labels.\n",
        "#     \"\"\"\n",
        "#     # Load data from files\n",
        "#     positive_examples = list(open(positive_data_file, \"r\", encoding='utf-8').readlines())\n",
        "#     positive_examples = [s.strip() for s in positive_examples]\n",
        "#     negative_examples = list(open(negative_data_file, \"r\", encoding='utf-8').readlines())\n",
        "#     negative_examples = [s.strip() for s in negative_examples]\n",
        "\n",
        "# def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "#     \"\"\"\n",
        "#     Generate a batch iterator for a dataset.\n",
        "#     \"\"\"\n",
        "#     data = np.array(data)\n",
        "#     data_size = len(data)\n",
        "#     num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
        "#     for epoch in range(num_epochs):\n",
        "#         # shuffle the data at each epoch\n",
        "#         if shuffle:\n",
        "#             shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "#             shuffled_data = data[shuffle_indices]\n",
        "#         else:\n",
        "#             shuffled_data = data\n",
        "#         for batch_num in range(num_batches_per_epoch):\n",
        "#             start_index = batch_num * batch_size\n",
        "#             end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "#             yield shuffled_data[start_index:end_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZvWYcDiJY0NO",
        "colab": {}
      },
      "source": [
        "def get_classes_results(testy, yhat_classes, yhat_probs):\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    from sklearn.metrics import precision_score\n",
        "    from sklearn.metrics import recall_score\n",
        "    from sklearn.metrics import f1_score\n",
        "    from sklearn.metrics import cohen_kappa_score\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    print(testy.shape)\n",
        "    print(yhat_classes.shape)\n",
        "    print(yhat_probs.shape)\n",
        "    try:\n",
        "        # accuracy: (tp + tn) / (p + n)\n",
        "        accuracy = accuracy_score(testy, yhat_classes)\n",
        "        print('Accuracy: %f' % accuracy)\n",
        "        # precision tp / (tp + fp)\n",
        "        precision = precision_score(testy, yhat_classes)\n",
        "        print('Precision: %f' % precision)\n",
        "        # recall: tp / (tp + fn)\n",
        "        recall = recall_score(testy, yhat_classes)\n",
        "        print('Recall: %f' % recall)\n",
        "        # f1: 2 tp / (2 tp + fp + fn)\n",
        "        f1 = f1_score(testy, yhat_classes)\n",
        "        print('F1 score: %f' % f1)\n",
        "    except:\n",
        "        try:\n",
        "            # kappa\n",
        "            kappa = cohen_kappa_score(testy, yhat_classes)\n",
        "            print('Cohens kappa: %f' % kappa)\n",
        "        except:\n",
        "            try:\n",
        "                # confusion matrix\n",
        "                matrix = confusion_matrix(testy, yhat_classes)\n",
        "                print(matrix)\n",
        "            except:\n",
        "                try:\n",
        "                    # ROC AUC\n",
        "                    auc = roc_auc_score(testy, yhat_probs)\n",
        "                    print('ROC AUC: %f' % auc)\n",
        "                except:\n",
        "                    # accuracy: (tp + tn) / (p + n)\n",
        "                    accuracy = accuracy_score(testy, yhat_probs)\n",
        "                    print('Accuracy: %f' % accuracy)\n",
        "\n",
        "def second_attempt_from_web(data_frame, text_vectorizer, class_vectorizer, classif_level, classif_type, dataset_location):\n",
        "    print('### CNN Doing Training ###')\n",
        "    date = datetime.now().isoformat()\n",
        "    \n",
        "    root_location = get_root_location('data/')\n",
        "    sequential_model_location = link_paths(join_paths(root_location, \"sequential_model/\"), \"sequential_cnn_model \"+date)\n",
        "    output_path = link_paths(join_paths(root_location, \"model_results/\"), \"cnn_results.csv\")\n",
        "    losses_path = link_paths(join_paths(root_location, \"training_losses/\"), \"cnn_losses.csv\")\n",
        "    \n",
        "    save_results = True\n",
        "\n",
        "    model_name = text_vectorizer+'/'+class_vectorizer+'/CNN'\n",
        "    standard_results = apply_df_vectorizer(data_frame, text_vectorizer, class_vectorizer, model_name)\n",
        "    train_data, test_data, train_labels, test_labels, classes, n_classes, vocab_processor, len_vocabulary = standard_results\n",
        "    # all ndarrays\n",
        "\n",
        "    train_data, val_data, train_labels, val_labels = get_train_test_from_data(train_data, train_labels)\n",
        "\n",
        "    save_sets(join_paths(root_location, 'model_sets/'), train_data, test_data, val_data, train_labels, test_labels, val_labels, [classes, n_classes, vocab_processor, len_vocabulary])\n",
        "\n",
        "    embedding_size = train_data.shape[1]\n",
        "\n",
        "    if text_vectorizer == 'load_doc2vec':\n",
        "        root_location = get_root_location('data/convolutional_outcome/')\n",
        "        doc2vec_vocab_location = link_paths(join_paths(root_location, \"doc2vec_model/vocab_model/\"), \"doc2vec_vocab_30_epochs\")\n",
        "        # doc2vec_vocab_location = link_paths(join_paths(root_location, \"doc2vec_model/vocab_model/\"), \"doc2vec_vocab\")\n",
        "        output_path = link_paths(join_paths(root_location, 'model_results/'), 'cnn_results.csv')\n",
        "\n",
        "        training_docs_list = train_data['patent_id']\n",
        "        val_docs_list = val_data['patent_id']\n",
        "        test_docs_list = test_data['patent_id']\n",
        "\n",
        "        sequence_size, embedding_size = 1, 200\n",
        "        # train_data, val_data, _ = get_df_data(2, training_docs_list, val_docs_list, None, sequence_size, embedding_size, doc2vec_vocab_location)\n",
        "        train_data, val_data, test_data = get_df_data(3, training_docs_list, val_docs_list, test_docs_list, sequence_size, embedding_size, doc2vec_vocab_location)  \n",
        "\n",
        "        vocab_processor = Doc2Vec.load(doc2vec_vocab_location)\n",
        "        len_vocabulary = len(vocab_processor.wv.vocab)\n",
        "    else:\n",
        "        train_data = np.reshape(train_data, (train_data.shape[0], 1, train_data.shape[1]))\n",
        "        test_data = np.reshape(test_data, (test_data.shape[0], 1, test_data.shape[1]))\n",
        "        val_data = np.reshape(val_data, (val_data.shape[0], 1, val_data.shape[1]))\n",
        "\n",
        "    print('train_data: ', train_data[0])\n",
        "    print('dataset shapes: ', train_data.shape, train_labels.shape, val_data.shape, val_labels.shape)\n",
        "\n",
        "    len_vocabulary = 57335\n",
        "    len_vocabulary = 34736\n",
        "\n",
        "    print(\"output path: \", output_path)\n",
        "    print(\"length vocabulary: \", len_vocabulary, \" embedding_size: \", embedding_size, \" num classes: \", n_classes)\n",
        "\n",
        "    parameters = {\"batch_size\": 64,\n",
        "                  \"optimizer\": \"Adam\",\n",
        "                  \"init_mode_1\": \"uniform\",\n",
        "                  \"init_mode_2\": \"uniform\",\n",
        "                  \"init_mode_3\": None,\n",
        "                  \"init_mode_4\": \"uniform\",\n",
        "                  \"init_mode_5\": \"uniform\",\n",
        "                  \"init_mode_6\": \"uniform\",\n",
        "                  \"activation_1\": \"relu\",\n",
        "                  \"activation_2\": \"relu\",\n",
        "                  \"activation_3\": None,\n",
        "                  \"activation_4\": \"softmax\",\n",
        "                  \"activation_5\": \"softmax\",\n",
        "                  \"activation_6\": \"softmax\",\n",
        "                  \"weight_constraint_1\": 3,\n",
        "                  \"weight_constraint_2\": 3,\n",
        "                  \"weight_constraint_3\": None,\n",
        "                  \"weight_constraint_4\": 3,\n",
        "                  \"weight_constraint_5\": 3,\n",
        "                  \"weight_constraint_6\": 3,\n",
        "                  \"dropout_rate_1\": 0.05,\n",
        "                  \"dropout_rate_2\": 0.1,\n",
        "                  \"dropout_rate_3\": 0.15,\n",
        "                  \"dropout_rate_4\": 0.2,\n",
        "                  \"neurons_1\": 910,\n",
        "                  # \"neurons_1\": 310,\n",
        "                  \"neurons_2\": 273.0,\n",
        "                  # \"neurons_2\": 93.0,\n",
        "                  \"neurons_3\": 182,\n",
        "                  # \"neurons_3\": 60,\n",
        "                  \"filters_1\": 200,\n",
        "                  # \"filters_1\": 16,\n",
        "                  \"filters_2\": 64,\n",
        "                  \"filters_3\": None,\n",
        "                  \"kernel_size_1\": 1,\n",
        "                  # \"kernel_size_1\": 16,\n",
        "                  \"kernel_size_2\": 64,\n",
        "                  \"kernel_size_3\": None,\n",
        "                  \"strides_1\": 1,\n",
        "                  \"strides_2\": 1,\n",
        "                  \"strides_3\": None\n",
        "                }\n",
        "\n",
        "    print(\"params: \", parameters)\n",
        "    model = get_cnn(parameters['optimizer'], \n",
        "                    parameters['init_mode_1'], parameters['activation_1'],\n",
        "                    parameters['init_mode_2'], parameters['activation_2'],\n",
        "                    parameters['init_mode_3'], parameters['activation_3'],\n",
        "                    parameters['init_mode_4'], parameters['activation_4'],\n",
        "                    parameters['init_mode_5'], parameters['activation_5'],\n",
        "                    parameters['init_mode_6'], parameters['activation_6'],\n",
        "                    parameters['weight_constraint_1'], \n",
        "                    parameters['weight_constraint_2'], \n",
        "                    parameters['weight_constraint_3'], \n",
        "                    parameters['weight_constraint_4'], \n",
        "                    parameters['weight_constraint_5'], \n",
        "                    parameters['weight_constraint_6'], \n",
        "                    parameters['dropout_rate_1'], \n",
        "                    parameters['dropout_rate_2'], \n",
        "                    parameters['dropout_rate_3'], \n",
        "                    parameters['dropout_rate_4'],\n",
        "                    parameters['neurons_1'], parameters['neurons_2'], parameters['neurons_3'],\n",
        "                    parameters['filters_1'], parameters['filters_2'], parameters['filters_3'],\n",
        "                    parameters['kernel_size_1'], parameters['kernel_size_2'], parameters['kernel_size_3'],\n",
        "                    parameters['strides_1'], parameters['strides_2'], parameters['strides_3'],\n",
        "                    len_vocabulary, embedding_size, n_classes)\n",
        "\n",
        "    print('original: ', val_data.shape)\n",
        "    try:\n",
        "        val_data = val_data[val_data.index <= (val_data.shape[0] // parameters['batch_size']) * parameters['batch_size']]\n",
        "        val_labels = val_labels[val_labels.index <= (val_labels.shape[0] // parameters['batch_size']) * parameters['batch_size']]\n",
        "    except:\n",
        "        val_data = val_data[:((val_data.shape[0] // parameters['batch_size']) * parameters['batch_size'])]\n",
        "        val_labels = val_labels[:((val_labels.shape[0] // parameters['batch_size']) * parameters['batch_size'])]   \n",
        "    print('shirnk: ', val_data.shape)\n",
        "\n",
        "    min_delta, patience = 0.00001, 50\n",
        "    early_stopper = EarlyStopping(monitor='val_loss', min_delta=min_delta, patience=patience, verbose=2, mode='auto')\n",
        "    metrics_callback = MetricsCNNCallback(val_data, val_labels, patience)\n",
        "    \n",
        "    # learning rate scheduling\n",
        "    loss_history = LossHistory(parameters, losses_path)\n",
        "    # lrate = LearningRateScheduler(exp_decay)\n",
        "    lrate = LearningRateScheduler(step_decay)\n",
        "\n",
        "    metrics, predictions, pred_classes, val_metrics = run_cnn(model, \n",
        "                                                              train_data, train_labels, val_data, val_labels, val_data, val_labels,\n",
        "                                                              parameters['batch_size'], \n",
        "                                                              [early_stopper, metrics_callback],\n",
        "                                                              True)\n",
        "                                                              # [loss_history, lrate, early_stopper, metrics_callback])\n",
        "    save_sequential_model(model, sequential_model_location)\n",
        "    print('model saved')    \n",
        "    \n",
        "    binary_predictions = get_binary_0_5(predictions)\n",
        "    \n",
        "    print('true values')\n",
        "    print(val_labels)\n",
        "    print(val_labels.shape)\n",
        "    print(val_labels[0])\n",
        "    print('predictions')\n",
        "    print(predictions)\n",
        "    print(predictions.shape)\n",
        "    print(predictions[0])\n",
        "    print('metrics - loss, acc, mse')\n",
        "    print(metrics)\n",
        "    # print('class predictions')\n",
        "    # print(pred_classes)\n",
        "    # print(pred_classes.shape)\n",
        "    # print(pred_classes[0])\n",
        "    print('binary predictions')\n",
        "    print(binary_predictions)\n",
        "    print(binary_predictions.shape)\n",
        "    print(binary_predictions[0])\n",
        "\n",
        "    classifier_name, parameters = get_sequential_classifier_information(model)\n",
        "    model_name = text_vectorizer+'/'+class_vectorizer+'/'+classifier_name\n",
        "\n",
        "    manual_metrics = calculate_manual_metrics(model_name, val_labels, binary_predictions)\n",
        "    none_average, binary_average, micro_average, macro_average = manual_metrics\n",
        "\n",
        "    # get_classes_results(val_labels, pred_classes, predictions)\n",
        "    get_classes_results(val_labels, pred_classes, binary_predictions)\n",
        "    \n",
        "    validation_metrics = get_sequential_metrics(val_labels, predictions, binary_predictions)\n",
        "    display_sequential_metrics(classifier_name, validation_metrics)\n",
        "    \n",
        "    if save_results:\n",
        "        save_results_function(classifier_name+' Val', validation_metrics, parameters, model_name, classif_level, classif_type, dataset_location)       \n",
        "    return model            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99iV0P4XbxKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_second_attempt_from_web(model, data_frame, text_vectorizer, class_vectorizer, classif_level, classif_type, dataset_location, date='01-01-2020'):\n",
        "    print('### CNN Doing Testing ###')\n",
        "    \n",
        "    root_location = get_root_location('data/')\n",
        "    sequential_model_location = link_paths(join_paths(root_location, \"sequential_model/\"), \"sequential_cnn_model \"+date)\n",
        "    output_path = link_paths(join_paths(root_location, \"model_results/\"), \"cnn_results.csv\")\n",
        "    losses_path = link_paths(join_paths(root_location, \"training_losses/\"), \"cnn_losses.csv\")\n",
        "    \n",
        "    model_name = text_vectorizer+'/'+class_vectorizer+'/CNN'\n",
        "    # results = apply_df_vectorizer(data_frame, text_vectorizer, class_vectorizer, model_name)\n",
        "    # X_train, X_test, y_train, y_test, classes, n_classes, vocab_processor, len_vocabulary = results\n",
        "\n",
        "    # X_train, X_val, y_train, y_val = get_train_test_from_data(X_train, y_train)\n",
        "\n",
        "    result_path = join_paths(root_location, \"model_results\")\n",
        "\n",
        "    X_train, Xt_data, Xv_data, train_labels, test_labels, val_labels, settings = load_sets(join_paths(root_location, 'model_sets/'))\n",
        "\n",
        "    embedding_size = X_train.shape[1]\n",
        "\n",
        "    if text_vectorizer == 'load_doc2vec':\n",
        "        root_location = get_root_location('data/convolutional_outcome/')\n",
        "        doc2vec_vocab_location = link_paths(join_paths(root_location, \"doc2vec_model/vocab_model/\"), \"doc2vec_vocab 2020-01-01\")\n",
        "        result_path = join_paths(root_location, \"model_results\")\n",
        "        output_path = link_paths(join_paths(root_location, \"model_results/\"), \"cnn_results.csv\")\n",
        "        losses_path = link_paths(join_paths(root_location, \"training_losses/\"), \"cnn_losses.csv\")\n",
        "        \n",
        "        training_docs_list = X_train['patent_id']\n",
        "        val_docs_list = Xv_data['patent_id']\n",
        "        test_docs_list = Xt_data['patent_id']\n",
        "        \n",
        "        sequence_size, embedding_size = 1, 150\n",
        "        train_data, val_data, test_data = get_df_data(3, training_docs_list, val_docs_list, test_docs_list, sequence_size, embedding_size, doc2vec_vocab_location)\n",
        "\n",
        "        vocab_processor = Doc2Vec.load(doc2vec_vocab_location)\n",
        "        len_vocabulary = len(vocab_processor.wv.vocab)\n",
        "  \n",
        "    save_results = True\n",
        "    \n",
        "    # train_data = train_data.reshape(train_data.shape[0], train_data.shape[2])\n",
        "    # val_data = val_data.reshape(train_data.shape[0], train_data.shape[2])\n",
        "    \n",
        "    parameters = {\"batch_size\": 64}\n",
        "    len_vocabulary = 57335\n",
        "    len_vocabulary = 34736\n",
        "    n_classes = train_labels.shape[1]\n",
        "\n",
        "    print(\"output path: \", output_path)\n",
        "    print(\"length vocabulary: \", len_vocabulary, \" number classes: \", n_classes)\n",
        "    \n",
        "    # model = load_sequential_model(sequential_model_location)\n",
        "    \n",
        "    print('original: ', test_data.shape)\n",
        "    try:\n",
        "        test_data = test_data[test_data.index <= (test_data.shape[0] // parameters['batch_size']) * parameters['batch_size']]\n",
        "        test_labels = test_labels[test_labels.index <= (test_labels.shape[0] // parameters['batch_size']) * parameters['batch_size']]\n",
        "    except:\n",
        "        test_data = test_data[:((test_data.shape[0] // parameters['batch_size']) * parameters['batch_size'])]\n",
        "        test_labels = test_labels[:((test_labels.shape[0] // parameters['batch_size']) * parameters['batch_size'])]   \n",
        "    print('shirnk: ', test_data.shape)\n",
        "\n",
        "    metrics, predictions, _, test_metrics = run_cnn(model, \n",
        "                                                    train_data, train_labels, test_data, test_labels, test_data, test_labels,\n",
        "                                                    parameters['batch_size'], None, \n",
        "                                                    False)\n",
        "    binary_predictions = get_binary_0_5(predictions)\n",
        "\n",
        "    classifier_name, parameters = get_sequential_classifier_information(model)\n",
        "    model_name = text_vectorizer+'/'+class_vectorizer+'/'+classifier_name\n",
        "    \n",
        "    metrics = get_sequential_metrics(test_labels, predictions, binary_predictions)\n",
        "    display_sequential_metrics(classifier_name, metrics)\n",
        "    \n",
        "    if save_results:\n",
        "        save_results_function(classifier_name+' Test', metrics, parameters, model_name, classif_level, classif_type, dataset_location)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKx3nK3xYhc7",
        "outputId": "43c0db43-243d-4521-f39d-8d9dc1544843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "def shrink_vocabulary(row, vocabulary, data_frame, ids_list):\n",
        "    if isinstance(row, pd.Series):\n",
        "        patent_id, text, classification = row.tolist()\n",
        "\n",
        "        new_tokens = [element for element in text if element in vocabulary]\n",
        "        if new_tokens != [] or len(new_tokens) > 2:\n",
        "            data_frame.loc[data_frame.shape[0]+1] = [patent_id, ' '.join(element for element in new_tokens), classification]\n",
        "        else:\n",
        "            ids_list.append(patent_id)\n",
        "\n",
        "def further_preprocessing_phase(temp_data_frame):\n",
        "    temp_data_frame['text'] = temp_data_frame['text'].apply(lambda text: th.tokenize_text(text) if text != None else '')\n",
        "    # textlist = temp_data_frame['text'].to_numpy()\n",
        "    textlist = temp_data_frame['text'].tolist()\n",
        "\n",
        "    # if it raises an exeption could be the empty texts\n",
        "    patent_dictionary = Dictionary(textlist)\n",
        "    corpus = [patent_dictionary.doc2bow(text) for text in textlist]\n",
        "\n",
        "    print('original dictionary size: ', len(patent_dictionary))\n",
        "\n",
        "    vocab_tf={}\n",
        "    for i in corpus:\n",
        "        for item, count in dict(i).items():\n",
        "            if item in vocab_tf:\n",
        "                vocab_tf[item]+=int(count)\n",
        "            else:\n",
        "                vocab_tf[item] =int(count)\n",
        "\n",
        "    remove_ids=[]\n",
        "    no_of_ids_below_limit=0\n",
        "    for id,count in vocab_tf.items():\n",
        "        if count<=5:\n",
        "            remove_ids.append(id)\n",
        "    patent_dictionary.filter_tokens(bad_ids=remove_ids)\n",
        "\n",
        "    patent_dictionary.filter_extremes(no_below=0)\n",
        "    patent_dictionary.filter_n_most_frequent(30)\n",
        "\n",
        "    print('parsed dictionary size: ', len(patent_dictionary))\n",
        "\n",
        "    vocabulary = list(patent_dictionary.token2id.keys())\n",
        "\n",
        "    ids_list = []\n",
        "    data_frame = pd.DataFrame(columns=['patent_id', 'text', 'classification'])\n",
        "    temp_data_frame.apply(lambda row : shrink_vocabulary(row, vocabulary, data_frame, ids_list), axis=1)\n",
        "    print(len(ids_list))\n",
        "    data_frame.set_index(data_frame['patent_id'], inplace=True)\n",
        "    data_frame.drop(ids_list, axis=0, inplace=True)\n",
        "    return data_frame\n",
        "\n",
        "def shrink_to_sectors(classcode_list):\n",
        "    new_list = []\n",
        "    for class_ in classcode_list:\n",
        "        if class_[0] not in new_list:\n",
        "            new_list.append(class_[0])\n",
        "    return ' '.join(element for element in new_list)\n",
        "\n",
        "def shrink_classes(df, row, class_list):\n",
        "    if isinstance(row, pd.Series):\n",
        "        patent_id, text, class_ = row.tolist()\n",
        "        new_classcodes = []\n",
        "        classcodes = tokenize_text(class_)\n",
        "        for classcode in classcodes:\n",
        "            if not classcode in class_list:\n",
        "                new_classcodes.append(classcode)\n",
        "        if new_classcodes != []:\n",
        "            new_class = ' '.join(el for el in new_classcodes)\n",
        "            df.loc[df.shape[0] + 1] = [patent_id, text, new_class]\n",
        "\n",
        "def reduce_amount_of_classes(data_frame, classification_df):\n",
        "    if isinstance(classification_df, pd.DataFrame):\n",
        "        threshold = int(data_frame.shape[0]/1000*0.35)\n",
        "        if threshold == 0:\n",
        "            threshold = 1\n",
        "        print('threshold for classcode: ', threshold)\n",
        "        temp_df = classifications_df[classifications_df['count'] <= threshold]\n",
        "        classes_list = temp_df['class'].tolist()\n",
        "    elif isinstance(classification_df, list):\n",
        "        classes_list = classification_df\n",
        "    df = pd.DataFrame(columns=['patent_id', 'text', 'classification'])\n",
        "    data_frame.apply(lambda row : shrink_classes(df, row, classes_list), axis=1)\n",
        "    return df, classes_list\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        # sys.argv.append('04_cnn_classify_patents.ipynb')\n",
        "        # sys.argv.append('/content/drive/My Drive/EP-Patent-Extraction/')\n",
        "        if len(sys.argv) == 5:\n",
        "            # source_path = 'test_clean/*/directories - and inside all the patents'\n",
        "            source_path = sys.argv[4]\n",
        "\n",
        "            # here the source_path must be passed as a string of the root directory of all data folders!\n",
        "            source_path, folder_level = handle_partial_args(source_path)\n",
        "        else:\n",
        "            sys.exit(1)\n",
        "    except:\n",
        "      source_path = ['/content/drive/My Drive/Colab Notebooks/data/test_classification/B/', \n",
        "                     '/content/drive/My Drive/Colab Notebooks/data/test_classification/B 1500 patents/']\n",
        "      source_path = ['/content/drive/My Drive/Colab Notebooks/data/test_classification/B 500 patents/']\n",
        "    \n",
        "    start = time.time()\n",
        "    print(source_path)\n",
        "\n",
        "    # text: here load_data/load_doc2vec means load the doc2vec vectors - needs a further layers: flatten (works with 1.x and 2.x, but doesn't work with keras), while load_data/standard means we do not apply any vectorizer (works with 1.x and 2.x).\n",
        "    # class: simple needs to know the set of shinked classes in advance while with keras i have to define the number of classes in advance\n",
        "    # if tensorflow does not have session, install tensorflow 2.0\n",
        "    # if define validation_steps, len_vocabulary == 0, \n",
        "    text_vectorizer = 'load_data/load_doc2vec'\n",
        "    class_vectorizer = 'multi_label'\n",
        "    classif_level = 'description_claim_abstract_title'\n",
        "    classif_type = 'subclasses'\n",
        "\n",
        "    # if there is the '/' into the text_vectorizer, you are supposed to load data from csv\n",
        "    index = text_vectorizer.rfind('/')\n",
        "    if index == -1:\n",
        "        # first option: here you read the directory with the patents\n",
        "        patent_ids, temp_df, classifications_df = load_data(source_path)\n",
        "        data_frame, classif_level, classif_type = get_final_df(patent_ids, temp_df, classif_type)\n",
        "\n",
        "        # data_frame, _ = reduce_amount_of_classes(data_frame, classifications_df)\n",
        "\n",
        "        # save_data_frame(script_key, data_frame, csvfile)\n",
        "    else:\n",
        "        # second option: here you specify the csv\n",
        "        csvfile = 'training_2000_2004_cleaned_2.csv'\n",
        "        # csvfile = 'training_2000_2004_cleaned_test.csv'\n",
        "        # csvfile = 'training_2000_2004_cleaned_test_middle.csv'\n",
        "        # save_data_frame(script_key, data_frame, csvfile)\n",
        "        classif_level, classif_type = 'description_claim_abstract_title', 'subclasses'\n",
        "        training_df, train_classifications_df = load_data_frame(script_key, csvfile)\n",
        "\n",
        "        csvfile = 'testing_2005_cleaned_2.csv'\n",
        "        # csvfile = 'testing_2005_cleaned_test.csv'\n",
        "        # csvfile = 'testing_2005_cleaned_test_smaller.csv'\n",
        "        testing_df, classifications_df = load_data_frame(script_key, csvfile)\n",
        "        \n",
        "        # train, classes_to_remove = reduce_amount_of_classes(training_df, train_classifications_df)\n",
        "        # test, _ = reduce_amount_of_classes(testing_df, classes_to_remove)\n",
        "\n",
        "        data_frame = pd.concat([training_df, testing_df])\n",
        "        patent_ids = data_frame['patent_id'].tolist()\n",
        "\n",
        "        text_vectorizer = text_vectorizer[index+1:]\n",
        "    \n",
        "    # around 50.000\n",
        "    # data_frame = further_preprocessing_phase(data_frame)\n",
        "\n",
        "    # data_frame['classification'] = data_frame['classification'].apply(lambda classcode : shrink_to_sectors(tokenize_text(classcode)))\n",
        "    # classification_df = pd.DataFrame(columns=['class', 'count'])\n",
        "    # data_frame['classification'].apply(lambda classcode : calculate_class_distribution(classcode, classification_df))\n",
        "    # print('sectors distribution : \\n', classification_df)\n",
        "\n",
        "    print('dataframe shape: ', data_frame.shape)\n",
        "\n",
        "    model = second_attempt_from_web(data_frame, text_vectorizer, class_vectorizer, classif_level, classif_type, source_path) # it should switch each class in fit and predict! - but it was written without the association - try to increase the epochs\n",
        "    \n",
        "    # whis would test the model on testing set\n",
        "    # test_second_attempt_from_web(model, data_frame, text_vectorizer, class_vectorizer, classif_level, classif_type, source_path)\n",
        "    \n",
        "    print(\"end cnn classification step, execution time: \", time.time()-start)\n",
        "\n",
        "# TODO:\n",
        "# the problem is the tuning of the model, how many (if we need it) conv-layers and which are\n",
        "#\n",
        "# TODO:\n",
        "# normalize values out of text vectorizer, such as for images they divided by 255.0 (max value?)\n",
        "#\n",
        "# keras.sequential.fit parameters\n",
        "#\n",
        "# convert text_cnn_model to multi label? image_cnn_model check if it is multi label or not\n",
        "#\n",
        "# google colab - to run my scripts faster on google servers\n",
        "# fuzzy with splitted vectorizer (abstract, description, claims) and another model for estimating the classes\n",
        "# !!!!! doc2vec has no tuned parameters, also the embedding variable needs to be changed from 200 to 150\n",
        "\n",
        "# embedding in cnn, val_samples in predict, should look why the sets are smaller after loading doc2vec?\n",
        "# , no, no\n",
        "# without few dense layers, without doc2vec\n",
        "# implement predict_classes https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/ - top1,top3,top5 - DONE\n",
        "# test metrics sklearn : scikit evaluation (binary, micro, macro) if they are close, there should be something wrong; 100/5 check with the text and not the vectors (machine learning models) - DONE\n",
        "# try to shrink the documents to the ones that have only classes with at least 100 documentes that belong to it - DONE\n",
        "\n",
        "# try one vs rest approach with cnn \n",
        "# try lstm with the new metrics - DONE\n",
        "# upload the new doc2vec model because i'm running with the old one // let's try with a balance distribution of classes (don't waste too much time)\n",
        "# fasttext try with the new distribution of classes \n",
        "# sectors new cleaned text data - leave the accuracy out // check out the link for improving the performance - DONE\n",
        "# change the distribution and amount of training (90%) and testing - beacause the reason may be the number of documents per class (same probabilities to belong to classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/My Drive/Colab Notebooks/data/test_classification/B 500 patents/']\n",
            "input_path:  /content/drive/My Drive/Colab Notebooks/model_dataframe/training_2000_2004_cleaned_2.csv\n",
            "dataframe shape:  (372759, 3)\n",
            "ids_list:  []\n",
            "dataframe shape:  (372759, 3)\n",
            "input_path:  /content/drive/My Drive/Colab Notebooks/model_dataframe/testing_2005_cleaned_2.csv\n",
            "dataframe shape:  (88230, 3)\n",
            "ids_list:  []\n",
            "dataframe shape:  (88230, 3)\n",
            "dataframe shape:  (460989, 3)\n",
            "### CNN Doing Training ###\n",
            "###  multi_label_binarizer  ###\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tavB5wlgzPX3"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(len_vocabulary, neurons_1))\n",
        "\n",
        "model.add(keras.layers.ZeroPadding1D((1,1), input_shape=(len_vocabulary, neurons_1)))\n",
        "model.add(keras.layers.Conv1D(filters=filters_1, kernel_size=kernel_size_1, strides=strides_1, activation=activation_1, kernel_initializer=init_mode_1, kernel_constraint=maxnorm(weight_constraint_1)))\n",
        "model.add(keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "model.add(keras.layers.ZeroPadding1D((1,1), input_shape=(len_vocabulary, neurons_1)))\n",
        "model.add(keras.layers.Conv1D(filters=filters_2, kernel_size=kernel_size_2, strides=strides_2, activation=activation_1, kernel_initializer=init_mode_1, kernel_constraint=maxnorm(weight_constraint_1)))\n",
        "model.add(keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dropout(dropout_rate_1))\n",
        "model.add(keras.layers.Dense(neurons_1, activation=activation, kernel_initializer=init_mode, kernel_constraint=maxnorm(weight_constraint_3)))\n",
        "model.add(keras.layers.Dropout(dropout_rate_2))\n",
        "model.add(keras.layers.Dense(neurons_2, activation=activation, kernel_initializer=init_mode, kernel_constraint=maxnorm(weight_constraint_4)))\n",
        "model.add(keras.layers.Dropout(dropout_rate_3))\n",
        "model.add(keras.layers.Dense(neurons_3, activation=activation, kernel_initializer=init_mode, kernel_constraint=maxnorm(weight_constraint_5)))\n",
        "model.add(keras.layers.Dropout(dropout_rate_4))\n",
        "model.add(keras.layers.Dense(n_classes, activation=tf.nn.sigmoid))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', 'mse'])\n",
        "\n",
        "best:\n",
        "\n",
        "need to test 1.15.0 and 2.1.0 with standard ---\n",
        "test keras binarizer, loss_history and lr scheduler --- \n",
        "test the ndarrays in this notebook ---\n",
        "edit the other scripts for last empty character\n"
      ]
    }
  ]
}